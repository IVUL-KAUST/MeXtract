{
    "metadata": {
        "Name": "FQuAD",
        "Link": "https://illuin-tech.github.io/FQuAD-explorer/",
        "HF Link": "",
        "License": "unknown",
        "Year": 2023,
        "Language": "fr",
        "Domain": [
            "wikipedia"
        ],
        "Form": "text",
        "Collection Style": [
            "human annotation"
        ],
        "Description": "FQuAD is a French Native Reading Comprehension dataset of questions and answers on a set of Wikipedia articles that consists of 25,000+ samples for the 1.0 version and 60,000+ samples for the 1.1 version.",
        "Volume": 60000.0,
        "Unit": "sentences",
        "Ethical Risks": "Low",
        "Provider": [
            "Illuin Technology",
            "ETH Zurich"
        ],
        "Derived From": [],
        "Paper Title": "FQuAD: French Question Answering Dataset",
        "Paper Link": "https://arxiv.org/abs/2002.06071",
        "Tokenized": false,
        "Host": "other",
        "Access": "Free",
        "Cost": "",
        "Test Split": true,
        "Tasks": [
            "question answering"
        ],
        "Venue Title": "",
        "Venue Type": "preprint",
        "Venue Name": "",
        "Authors": [
            "Martin d'Hoffschmidt",
            "Wacim Belblidia",
            "Tom Brendl\u00e9",
            "Quentin Heinrich",
            "Maxime Vidal"
        ],
        "Affiliations": [
            "Illuin Technology",
            "ETH Zurich"
        ],
        "Abstract": "Recent advances in the field of language modeling have improved state-of-the-art results on many Natural Language Processing tasks. Among them, Reading Comprehension has made significant progress over the past few years. However, most results are reported in English since labeled resources available in other languages, such as French, remain scarce. In the present work, we introduce the French Question Answering Dataset (FQuAD). FQuAD is a French Native Reading Comprehension dataset of questions and answers on a set of Wikipedia articles that consists of 25,000+ samples for the 1.0 version and 60,000+ samples for the 1.1 version. We train a baseline model which achieves an F1 score of 92.2 and an exact match ratio of 82.1 on the test set. In order to track the progress of French Question Answering models we propose a leader-board and we have made the 1.0 version of our dataset freely available at https://illuin-tech.github.io/FQuAD-explorer/. The release of a native French Reading Comprehension dataset is motivated by the release of recent French monolingual models (CamemBERT, FlauBERT) and by industrial opportunities. In addition to that, we think that a French dataset opens up a wide range of possible experiments at the research level. First, while it is generally accepted that monolingual models perform better than multilingual models we find that the gap is narrower than expected for the Reading Comprehension task. Second, to fine-tune a model on a target language, translated datasets have been extensively used but the lack of native data to evaluate the approach, at least in French, makes it difficult to evaluate it. Third, apart from Question Answering models for French applications, cross-lingual applications have found significant interest recently with XQuAD and MLQA where the need for quality annotated data on other languages than English are important to evaluate how models transfer across languages."
    },
    "validation": {
        "ACCESSABILITY": 0.5714285714285714,
        "DIVERSITY": 1.0,
        "CONTENT": 0.8571428571428571,
        "EVALUATION": 0.6666666666666666,
        "AVERAGE": 0.7222222222222222
    },
    "length_forcing": 0.9999999999999996,
    "cost": {
        "cost": 0.0444775,
        "input_tokens": 14760,
        "output_tokens": 757
    },
    "config": {
        "model_name": "openai_gpt-4o",
        "few_shot": 0,
        "month": null,
        "year": "2020",
        "keywords": [
            ""
        ],
        "link": "https://arxiv.org/abs/2002.06071"
    },
    "ratio_filling": 1.0,
    "error": null
}