{
    "metadata": {
        "Name": "TriviaQA",
        "Link": "http://nlp.cs.washington.edu/triviaqa/",
        "HF Link": "",
        "License": "unknown",
        "Year": 2017,
        "Language": "en",
        "Domain": [
            "web pages",
            "wikipedia"
        ],
        "Form": "text",
        "Collection Style": [
            "manual curation",
            "human annotation"
        ],
        "Description": "A large-scale reading comprehension dataset with over 650K question-answer-evidence triples from trivia questions with supporting evidence documents from Wikipedia and web sources",
        "Volume": 650000.0,
        "Unit": "documents",
        "Ethical Risks": "Low",
        "Provider": [
            "University of Washington"
        ],
        "Derived From": [],
        "Paper Title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension",
        "Paper Link": "https://arxiv.org/abs/1705.03551",
        "Tokenized": false,
        "Host": "other",
        "Access": "Free",
        "Cost": "",
        "Test Split": true,
        "Tasks": [
            "question answering",
            "machine translation"
        ],
        "Venue Title": "",
        "Venue Type": "conference",
        "Venue Name": "",
        "Authors": [
            "Mandar Joshi",
            "Eunsol Choi",
            "Daniel Weld",
            "Luke Zettlemoyer"
        ],
        "Affiliations": [
            "University of Washington"
        ],
        "Abstract": "We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA has relatively complex, compositional questions, has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, demonstrating the difficulty of the dataset and leaving ample room for future improvement."
    },
    "validation": {
        "ACCESSABILITY": 0.5714285714285714,
        "DIVERSITY": 1.0,
        "CONTENT": 0.8571428571428571,
        "EVALUATION": 0.6666666666666666,
        "AVERAGE": 0.7222222222222222
    },
    "length_forcing": 0.9999999999999996,
    "cost": {
        "cost": 0.027753,
        "input_tokens": 5569,
        "output_tokens": 445
    },
    "config": {
        "model_name": "anthropic_claude-3.5-sonnet",
        "few_shot": 0,
        "month": null,
        "year": "2017",
        "keywords": [
            ""
        ],
        "link": "https://arxiv.org/abs/1705.03551"
    },
    "ratio_filling": 1.0,
    "error": null
}