{
    "metadata": {
        "Name": "POLYGLOT-NER",
        "Subsets": [],
        "Link": "https://github.com/rami-al-rfou/polyglot_ner",
        "HF Link": "",
        "License": "unknown",
        "Year": 2013,
        "Language": "multilingual",
        "Dialect": "mixed",
        "Domain": [
            "wikipedia"
        ],
        "Form": "text",
        "Collection Style": [
            "machine annotation"
        ],
        "Description": "A multilingual NER system for 40 languages with no language specific knowledge or expertise using automatically learned features and language agnostic data processing techniques",
        "Volume": 40.0,
        "Unit": "sentences",
        "Ethical Risks": "Low",
        "Provider": [
            "Stony Brook University"
        ],
        "Derived From": [
            "Wikipedia",
            "Freebase"
        ],
        "Paper Title": "POLYGLOT-NER: Massive Multilingual Named Entity Recognition",
        "Paper Link": "https://arxiv.org/abs/1410.3791",
        "Script": "Arab",
        "Tokenized": false,
        "Host": "GitHub",
        "Access": "Free",
        "Cost": "",
        "Test Split": true,
        "Tasks": [
            "named entity recognition"
        ],
        "Venue Title": "CoNLL",
        "Venue Type": "conference",
        "Venue Name": "Conference on Natural Language Learning",
        "Authors": [
            "Rami Al-Rfou",
            "Bryan Perozzi",
            "Vivek Kulkarni",
            "Steven Skiena"
        ],
        "Affiliations": [
            "Department of Computer Science Stony Brook University"
        ],
        "Abstract": "The increasing diversity of languages used on the web introduces a new level of complexity to Information Retrieval (IR) systems. We can no longer assume that textual content is written in one language or even the same language family. In NLP and Information Retrieval (IR) systems, where it is used for a variety of purposes (e.g, event extraction or knowledge base population). Successful approaches to address NER rely on supervised learning. Applying these approaches to a massively multilingual setting exposes two major drawbacks; First, they require human annotated datasets which are scarce. Second, to design relevant features, sufficient linguistic proficiency is required for each language of interest. This makes building multilingual NER annotators a tedious and cumbersome process. Our method learns distributed word representations (word embeddings) which encode semantic and syntactic features of words in each language. Then, we automatically generate datasets from Wikipedia link structure and Freebase attributes. Finally, we apply two preprocessing stages (oversampling and exact surface form matching) which do not require any linguistic expertise. In this paper, we demonstrate how to build massive multilingual annotators with minimal human expertise and intervention. We describe a system that builds Named Entity Recognition (NER) annotators for 40 major languages using Wikipedia and Freebase. Our approach does not require NER human annotated datasets or language specific resources like treebanks, parallel corpora, and orthographic rules. The novelty of approach lies therein - using only language agnostic techniques, while achieving competitive performance."
    },
    "validation": {
        "DIVERSITY": 0.6666666666666666,
        "ACCESSABILITY": 0.5714285714285714,
        "CONTENT": 0.75,
        "EVALUATION": 0.3333333333333333,
        "AVERAGE": 0.6190476190476191
    },
    "length_forcing": 1.0,
    "cost": {
        "cost": 0.00324258,
        "input_tokens": 15946,
        "output_tokens": 635
    },
    "config": {
        "model_name": "meta-llama_llama-4-maverick",
        "few_shot": 0,
        "month": null,
        "year": "2014",
        "keywords": [
            ""
        ],
        "link": "https://arxiv.org/abs/1410.3791"
    },
    "ratio_filling": 1.0,
    "error": null
}