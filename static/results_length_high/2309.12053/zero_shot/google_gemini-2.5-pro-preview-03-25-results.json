{
    "metadata": {
        "Name": "AceGPT",
        "Subsets": [
            {
                "Name": "Arabic pre-training corpus for AceGPT-7B",
                "Volume": 19200000000.0,
                "Unit": "tokens",
                "Dialect": "Modern Standard Arabic"
            },
            {
                "Name": "Arabic pre-training corpus for AceGPT-13B",
                "Volume": 6000000000.0,
                "Unit": "tokens",
                "Dialect": "Modern Standard Arabic"
            },
            {
                "Name": "Quora-Arabic-40K (SFT)",
                "Volume": 43050.0,
                "Unit": "sentences",
                "Dialect": "Modern Standard Arabic"
            },
            {
                "Name": "Alpaca-Arabic (SFT)",
                "Volume": 49969.0,
                "Unit": "sentences",
                "Dialect": "Modern Standard Arabic"
            },
            {
                "Name": "Evol-Instruct-Arabic (SFT)",
                "Volume": 69997.0,
                "Unit": "sentences",
                "Dialect": "Modern Standard Arabic"
            },
            {
                "Name": "Code-Alpaca-Arabic (SFT)",
                "Volume": 20022.0,
                "Unit": "sentences",
                "Dialect": "Modern Standard Arabic"
            }
        ],
        "Link": "https://github.com/FreedomIntelligence/AceGPT",
        "HF Link": "https://huggingface.co/datasets/FreedomIntelligence/AceGPT-ACVA",
        "License": "Apache-2.0",
        "Year": 2023,
        "Language": "multilingual",
        "Dialect": "Modern Standard Arabic",
        "Domain": [
            "web pages",
            "wikipedia",
            "social media"
        ],
        "Form": "text",
        "Collection Style": [
            "crawling",
            "LLM generated",
            "manual curation"
        ],
        "Description": "AceGPT: Arabic LLM via localized pre-training, SFT, RLAIF for cultural alignment.",
        "Volume": 19200000000.0,
        "Unit": "tokens",
        "Ethical Risks": "Medium",
        "Provider": [
            "The Chinese University of Hong Kong, Shenzhen",
            "King Abdullah University of Science and Technology"
        ],
        "Derived From": [
            "LLaMA2",
            "Arabic text 2022",
            "Arabic Wikipedia",
            "CC100",
            "OSCAR3",
            "Slim Pajama",
            "Alpaca",
            "Evol-Instruct",
            "Code-Alpaca",
            "ShareGPT",
            "Quora"
        ],
        "Paper Title": "AceGPT, Localizing Large Language Models in Arabic",
        "Paper Link": "https://arxiv.org/abs/2311.00709",
        "Script": "Arab",
        "Tokenized": false,
        "Host": "GitHub",
        "Access": "Free",
        "Cost": "",
        "Test Split": true,
        "Tasks": [
            "instruction tuning"
        ],
        "Venue Title": "arXiv",
        "Venue Type": "preprint",
        "Venue Name": "arXiv",
        "Authors": [
            "Huang Huang",
            "Fei Yu",
            "Jianqing Zhu",
            "Xuening Sun",
            "Hao Cheng",
            "Dingjie Song",
            "Zhihong Chen",
            "Abdulmohsen Alharthi",
            "Bang An",
            "Juncai He",
            "Ziche Liu",
            "Zhiyi Zhang",
            "Junying Chen",
            "Jianquan Li",
            "Benyou Wang",
            "Lian Zhang",
            "Ruoyu Sun",
            "Xiang Wan",
            "Haizhou Li",
            "Jinchao Xu"
        ],
        "Affiliations": [
            "Shenzhen International Center for Industrial and Applied Mathematics, Shenzhen Research Institute of Big Data",
            "The Chinese University of Hong Kong, Shenzhen, China",
            "King Abdullah University of Science and Technology, Thuwal, Saudi Arabia",
            "Shenzhen Research Institute of Big Data, Shenzhen, China"
        ],
        "Abstract": "This paper is devoted to the development of a localized Large Language Model (LLM) specifically for Arabic, a language imbued with unique cultural characteristics inadequately addressed by current mainstream models. Significant concerns emerge when addressing cultural sensitivity and local values. To address this, the paper proposes a comprehensive solution that includes further pre-training with Arabic texts, Supervised Fine-Tuning (SFT) utilizing native Arabic instructions, and GPT-4 responses in Arabic, alongside Reinforcement Learning with AI Feedback (RLAIF) employing a reward model attuned to local culture and values. The goal is to cultivate culturally cognizant and value-aligned Arabic LLMs capable of accommodating the diverse, application-specific needs of Arabic-speaking communities. Comprehensive evaluations reveal that the resulting model, dubbed 'AceGPT', sets the state-of-the-art standard for open Arabic LLMs across various benchmarks. Codes, data, and models are in https://github.com/FreedomIntelligence/AceGPT."
    },
    "validation": {
        "DIVERSITY": 0.3333333333333333,
        "ACCESSABILITY": 0.7142857142857143,
        "CONTENT": 0.375,
        "EVALUATION": 0.0,
        "AVERAGE": 0.42857142857142855
    },
    "length_forcing": 1.0,
    "cost": {
        "cost": 0.13005,
        "input_tokens": 34478,
        "output_tokens": 1128
    },
    "config": {
        "model_name": "google_gemini-2.5-pro-preview-03-25",
        "few_shot": 0,
        "month": null,
        "year": "2023",
        "keywords": [
            ""
        ],
        "link": "https://arxiv.org/abs/2309.12053"
    },
    "ratio_filling": 1.0,
    "error": null
}