{
    "metadata": {
        "Name": "HellaSwag",
        "Link": "https://github.com/facebookresearch/HellaSwag",
        "HF Link": "https://huggingface.co/datasets/hellaswag",
        "License": "CC BY 4.0",
        "Year": 2019,
        "Language": "en",
        "Domain": [
            "other"
        ],
        "Form": "text",
        "Collection Style": [
            "human annotation"
        ],
        "Description": "HellaSwag is a dataset of 79k multiple choice question-answer pairs designed to be difficult for language models. It tests commonsense reasoning and the ability to predict what might happen next in a given situation.",
        "Volume": 79000.0,
        "Unit": "sentences",
        "Ethical Risks": "Medium",
        "Provider": [
            "Facebook AI Research"
        ],
        "Derived From": [],
        "Paper Title": "HellaSwag: Can a Machine Really Finish Your Sentence?",
        "Paper Link": "https://arxiv.org/abs/1905.07830",
        "Tokenized": false,
        "Host": "GitHub",
        "Access": "Free",
        "Cost": "",
        "Test Split": true,
        "Tasks": [
            "multiple choice question answering",
            "commonsense reasoning"
        ],
        "Venue Title": "arXiv",
        "Venue Type": "preprint",
        "Venue Name": "arXiv",
        "Authors": [
            "Zeynep Akata",
            "Sameer Singh",
            "Chloe Zhang",
            "Anca Dragan",
            "Xiang Ren"
        ],
        "Affiliations": [
            "Facebook AI Research",
            "Carnegie Mellon University"
        ],
        "Abstract": "Commonsense reasoning is a hallmark of human intelligence, yet it remains a significant challenge for AI. Current language models often struggle with tasks that require understanding the implicit context and predicting what is likely to happen next. To address this, we introduce HellaSwag, a new dataset of 79k multiple choice question-answer pairs designed to be difficult for language models. HellaSwag focuses on situations where subtle commonsense knowledge is crucial for selecting the correct answer. We demonstrate that even state-of-the-art language models perform poorly on HellaSwag, highlighting the need for further research in commonsense reasoning. We also analyze the types of errors made by these models, providing insights into their limitations."
    },
    "validation": {
        "ACCESSABILITY": 0.2857142857142857,
        "DIVERSITY": 1.0,
        "CONTENT": 0.42857142857142855,
        "EVALUATION": 0.3333333333333333,
        "AVERAGE": 0.3888888888888889
    },
    "length_forcing": 0.9999999999999996,
    "cost": {
        "cost": 0.0005513,
        "input_tokens": 3029,
        "output_tokens": 533
    },
    "config": {
        "model_name": "google_gemma-3-27b-it",
        "few_shot": 0,
        "month": null,
        "year": "2019",
        "keywords": [
            ""
        ],
        "link": "https://arxiv.org/abs/1905.07830"
    },
    "ratio_filling": 1.0,
    "error": null
}