{
    "metadata": {
        "Name": "FrenchToxicityPrompts",
        "Link": "https://huggingface.co/datasets/pierreguillou/french-toxicity-prompts",
        "HF Link": "https://huggingface.co/datasets/pierreguillou/french-toxicity-prompts",
        "License": "CC BY-NC-SA 4.0",
        "Year": 2023,
        "Language": "fr",
        "Domain": [
            "social media"
        ],
        "Form": "text",
        "Collection Style": [
            "crawling"
        ],
        "Description": "This dataset is a large benchmark for evaluating and mitigating toxicity in French texts. It contains prompts designed to elicit toxic responses from language models.",
        "Volume": 10000.0,
        "Unit": "sentences",
        "Ethical Risks": "Medium",
        "Provider": [
            "Pierre Guillou"
        ],
        "Derived From": [],
        "Paper Title": "FrenchToxicityPrompts: a Large Benchmark for Evaluating and Mitigating Toxicity in French Texts",
        "Paper Link": "https://arxiv.org/abs/2311.16459",
        "Tokenized": false,
        "Host": "HuggingFace",
        "Access": "Free",
        "Cost": "",
        "Test Split": false,
        "Tasks": [
            "offensive language detection",
            "topic classification"
        ],
        "Venue Title": "arXiv",
        "Venue Type": "preprint",
        "Venue Name": "arXiv",
        "Authors": [
            "Pierre Guillou"
        ],
        "Affiliations": [],
        "Abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in generating human-quality text. However, they can also generate toxic content, which raises serious ethical concerns. Evaluating and mitigating toxicity in LLMs is therefore crucial. Existing benchmarks for evaluating toxicity are primarily focused on English. In this paper, we introduce FrenchToxicityPrompts, a large benchmark for evaluating and mitigating toxicity in French texts. The benchmark consists of 10,000 prompts designed to elicit toxic responses from LLMs. We evaluate several state-of-the-art LLMs on the benchmark and show that they all generate toxic content to some extent. We also propose a simple method for mitigating toxicity in LLMs, which reduces the amount of toxic content generated by the models. The dataset and code are publicly available at https://huggingface.co/datasets/pierreguillou/french-toxicity-prompts."
    },
    "validation": {
        "ACCESSABILITY": 0.2857142857142857,
        "DIVERSITY": 1.0,
        "CONTENT": 0.5714285714285714,
        "EVALUATION": 0.6666666666666666,
        "AVERAGE": 0.5
    },
    "length_forcing": 0.9999999999999996,
    "cost": {
        "cost": 0.00042793,
        "input_tokens": 3026,
        "output_tokens": 557
    },
    "config": {
        "model_name": "google_gemma-3-27b-it",
        "few_shot": 0,
        "month": null,
        "year": "2024",
        "keywords": [
            ""
        ],
        "link": "https://arxiv.org/abs/2406.17566"
    },
    "ratio_filling": 1.0,
    "error": null
}