{
    "metadata": {
        "Name": "SQuAD 2.0",
        "Link": "https://rajpurkar.github.io/SQuAD-explorer/",
        "HF Link": "https://huggingface.co/datasets/squad_v2",
        "License": "CC BY-SA 4.0",
        "Year": 2018,
        "Language": "en",
        "Domain": [
            "wikipedia"
        ],
        "Form": "text",
        "Collection Style": [
            "human annotation"
        ],
        "Description": "SQuAD 2.0 combines the 100,000 questions from SQuAD 1.1 with over 50,000 new, unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To perform well, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering.",
        "Volume": 142192.0,
        "Unit": "sentences",
        "Ethical Risks": "Low",
        "Provider": [
            "Stanford University"
        ],
        "Derived From": [
            "SQuAD 1.1",
            "Wikipedia"
        ],
        "Paper Title": "Know What You Don't Know: Unanswerable Questions for SQuAD",
        "Paper Link": "https://arxiv.org/abs/1806.03822",
        "Tokenized": false,
        "Host": "GitHub",
        "Access": "Free",
        "Cost": "",
        "Test Split": true,
        "Tasks": [
            "question answering"
        ],
        "Venue Title": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
        "Venue Type": "conference",
        "Venue Name": "Association for Computational Linguistics",
        "Authors": [
            "Pranav Rajpurkar",
            "Robin Jia",
            "Percy Liang"
        ],
        "Affiliations": [
            "Stanford University"
        ],
        "Abstract": "Existing question answering systems are trained on datasets that assume that an answer to every question is present in the accompanying text. This is an unrealistic assumption for real-world applications. To address this, we present SQuAD 2.0, the latest version of the Stanford Question Answering Dataset (SQuAD). SQuAD 2.0 combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD 2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuAD 2.0 is a challenging natural language understanding task for existing models: a strong neural system that gets 86% F1 on SQuAD 1.1 achieves only 66% F1 on SQuAD 2.0. We hope that SQuAD 2.0 will spur the development of new models for reading comprehension that know what they don't know."
    },
    "validation": {
        "ACCESSABILITY": 0.8571428571428571,
        "DIVERSITY": 1.0,
        "CONTENT": 0.8571428571428571,
        "EVALUATION": 1.0,
        "AVERAGE": 0.8888888888888888
    },
    "length_forcing": 0.9655172413793099,
    "cost": {
        "cost": 0.04163375,
        "input_tokens": 3018,
        "output_tokens": 1454
    },
    "config": {
        "model_name": "google_gemini-2.5-pro-preview-03-25",
        "few_shot": 0,
        "month": null,
        "year": "2018",
        "keywords": [
            ""
        ],
        "link": "https://arxiv.org/abs/1806.03822"
    },
    "ratio_filling": 1.0,
    "error": null
}