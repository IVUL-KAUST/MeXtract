{
    "metadata": {
        "Name": "TriviaQA",
        "Link": "https://huggingface.co/datasets/trivia_qa",
        "HF Link": "https://huggingface.co/datasets/trivia_qa",
        "License": "Apache-2.0",
        "Year": 2017,
        "Language": "en",
        "Domain": [
            "web pages",
            "wikipedia"
        ],
        "Form": "text",
        "Collection Style": [
            "crawling",
            "machine annotation"
        ],
        "Description": "A large-scale reading comprehension dataset containing over 650K question-answer-evidence triples. Question-answer pairs are authored by trivia enthusiasts and collected from websites. Evidence documents are retrieved from Wikipedia and a web crawl, and are distantly supervised.",
        "Volume": 95457.0,
        "Unit": "documents",
        "Ethical Risks": "Low",
        "Provider": [
            "University of Washington"
        ],
        "Derived From": [],
        "Paper Title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension",
        "Paper Link": "https://aclanthology.org/P17-1147/",
        "Tokenized": false,
        "Host": "HuggingFace",
        "Access": "Free",
        "Cost": "",
        "Test Split": true,
        "Tasks": [
            "question answering"
        ],
        "Venue Title": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
        "Venue Type": "conference",
        "Venue Name": "Association for Computational Linguistics",
        "Authors": [
            "Mandar Joshi",
            "Eunsol Choi",
            "Daniel S. Weld",
            "Luke Zettlemoyer"
        ],
        "Affiliations": [
            "Paul G. Allen School of Computer Science & Engineering, University of Washington"
        ],
        "Abstract": "We introduce TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in contrast to other recently introduced datasets, TriviaQA questions are compositional and require non-trivial reasoning. Our dataset is constructed to be a realistic and difficult challenge for reading comprehension. We also present a detailed analysis of the dataset and compare it to other reading comprehension datasets. To benchmark the performance of current reading comprehension systems on TriviaQA, we evaluate a state-of-the-art system and show that it achieves a score of 55.3% on our dataset, which is a promising start but far from human performance. We release our dataset to the community to inspire future work on this challenging task."
    },
    "validation": {
        "ACCESSABILITY": 0.5714285714285714,
        "DIVERSITY": 1.0,
        "CONTENT": 0.7142857142857143,
        "EVALUATION": 1.0,
        "AVERAGE": 0.7222222222222222
    },
    "length_forcing": 0.9999999999999996,
    "cost": {
        "cost": 0.0444625,
        "input_tokens": 3021,
        "output_tokens": 1465
    },
    "config": {
        "model_name": "google_gemini-2.5-pro-preview-03-25",
        "few_shot": 0,
        "month": null,
        "year": "2017",
        "keywords": [
            ""
        ],
        "link": "https://arxiv.org/abs/1705.03551"
    },
    "ratio_filling": 1.0,
    "error": null
}