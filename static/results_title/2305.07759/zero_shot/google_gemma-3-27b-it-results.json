{
    "metadata": {
        "Name": "TinyStories",
        "Link": "https://github.com/tiny-stories/tiny-stories",
        "HF Link": "https://huggingface.co/datasets/tiny-stories",
        "License": "CC BY 4.0",
        "Year": 2023,
        "Language": "en",
        "Domain": [
            "other"
        ],
        "Form": "text",
        "Collection Style": [
            "LLM generated"
        ],
        "Description": "TinyStories is a dataset of 11k short stories generated by a language model. The stories are designed to be simple enough for small language models to learn from.",
        "Volume": 11000.0,
        "Unit": "documents",
        "Ethical Risks": "Low",
        "Provider": [
            "together computer"
        ],
        "Derived From": [],
        "Paper Title": "TinyStories: How Small Can Language Models Be and Still Speak Coherent English?",
        "Paper Link": "https://arxiv.org/abs/2305.10592",
        "Tokenized": false,
        "Host": "GitHub",
        "Access": "Free",
        "Cost": "",
        "Test Split": false,
        "Tasks": [
            "language modeling",
            "text generation"
        ],
        "Venue Title": "arXiv",
        "Venue Type": "preprint",
        "Venue Name": "arXiv",
        "Authors": [
            "Jean-Eudes Ranville",
            "Baptiste Rozi\u00e8re",
            "R\u00e9mi Lebreton",
            "Jules Carpentier",
            "Thomas Scialom"
        ],
        "Affiliations": [],
        "Abstract": "We study the limits of language model size by training models with as few as 10 million parameters. We find that even these small models can generate coherent English stories when trained on a carefully curated dataset of 11k short stories, TinyStories. We analyze the capabilities of these models and show that they can perform simple reasoning tasks. We also find that the performance of these models is sensitive to the training data and the model architecture. Our results suggest that it is possible to build useful language models with significantly fewer parameters than current state-of-the-art models."
    },
    "validation": {
        "ACCESSABILITY": 0.2857142857142857,
        "DIVERSITY": 1.0,
        "CONTENT": 0.7142857142857143,
        "EVALUATION": 1.0,
        "AVERAGE": 0.6111111111111112
    },
    "length_forcing": 0.9999999999999996,
    "cost": {
        "cost": 0.00041671,
        "input_tokens": 3018,
        "output_tokens": 484
    },
    "config": {
        "model_name": "google_gemma-3-27b-it",
        "few_shot": 0,
        "month": null,
        "year": "2023",
        "keywords": [
            ""
        ],
        "link": "https://arxiv.org/abs/2305.07759"
    },
    "ratio_filling": 1.0,
    "error": null
}