{
    "metadata": {
        "Name": "TinyStories",
        "Link": "https://huggingface.co/datasets/roneneldan/TinyStories",
        "HF Link": "https://huggingface.co/datasets/roneneldan/TinyStories",
        "License": "CC BY 4.0",
        "Year": 2023,
        "Language": "en",
        "Domain": [
            "LLM"
        ],
        "Form": "text",
        "Collection Style": [
            "LLM generated"
        ],
        "Description": "A synthetic dataset of simple short stories generated by GPT-3.5 and GPT-4, using a vocabulary understandable by a young child. It is intended for training small language models to produce coherent English.",
        "Volume": 800000000.0,
        "Unit": "documents",
        "Ethical Risks": "Low",
        "Provider": [
            "Microsoft Research"
        ],
        "Derived From": [],
        "Paper Title": "TinyStories: How Small Can Language Models Be and Still Speak Coherent English?",
        "Paper Link": "https://arxiv.org/abs/2305.07759",
        "Tokenized": false,
        "Host": "HuggingFace",
        "Access": "Free",
        "Cost": "",
        "Test Split": true,
        "Tasks": [
            "language modeling",
            "text generation"
        ],
        "Venue Title": "arXiv",
        "Venue Type": "preprint",
        "Venue Name": "arXiv",
        "Authors": [
            "Ronen Eldan",
            "Yuanzhi Li"
        ],
        "Affiliations": [
            "Microsoft Research"
        ],
        "Abstract": "We introduce TinyStories, a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-old child understands, generated by GPT-3.5 and GPT-4. The goal is to create a dataset for training and evaluating small language models that are capable of producing coherent and fluent English. We find that LMs with as few as 10 million parameters, and a sufficiently small vocabulary, can be trained on this dataset to the point of producing fluent and consistent stories with several paragraphs that are diverse and have a coherent plot. The models learn grammar, and acquire a 'story-telling' ability to create novel stories with characters and events. For example, a 28M-parameter model can produce stories that are rated as better than those of GPT-3.5 by human evaluators. We also show that the models can be prompted at inference time to produce stories with specific contents, a-la instruction-following. We release the dataset, the models, and the evaluation code."
    },
    "validation": {
        "ACCESSABILITY": 0.8571428571428571,
        "DIVERSITY": 1.0,
        "CONTENT": 0.8571428571428571,
        "EVALUATION": 0.6666666666666666,
        "AVERAGE": 0.8333333333333334
    },
    "length_forcing": 0.9999999999999996,
    "cost": {
        "cost": 0.05042375,
        "input_tokens": 3018,
        "output_tokens": 1577
    },
    "config": {
        "model_name": "google_gemini-2.5-pro-preview-03-25",
        "few_shot": 0,
        "month": null,
        "year": "2023",
        "keywords": [
            ""
        ],
        "link": "https://arxiv.org/abs/2305.07759"
    },
    "ratio_filling": 1.0,
    "error": null
}