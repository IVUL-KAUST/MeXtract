{
    "metadata": {
        "Name": "Bright",
        "Link": "https://github.com/xlang-ai/BRIGHT",
        "HF Link": "https://huggingface.co/datasets/xlangai/BRIGHT",
        "License": "unknown",
        "Year": 2024,
        "Language": "multilingual",
        "Domain": [
            "social media",
            "web pages",
            "books"
        ],
        "Form": "text",
        "Collection Style": [
            "crawling",
            "human annotation"
        ],
        "Description": "A new benchmark that requires intensive reasoning to retrieve relevant documents from diverse domains.",
        "Volume": 1384.0,
        "Unit": "documents",
        "Ethical Risks": "Medium",
        "Provider": [
            "Princeton University",
            "The University of Hong Kong",
            "Google"
        ],
        "Derived From": [],
        "Paper Title": "Bright: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval",
        "Paper Link": "https://arxiv.org/abs/2405.18891",
        "Tokenized": false,
        "Host": "HuggingFace",
        "Access": "Free",
        "Cost": "",
        "Test Split": true,
        "Tasks": [
            "question answering",
            "information retrieval"
        ],
        "Venue Title": "ICLR",
        "Venue Type": "conference",
        "Venue Name": "International Conference on Learning Representations",
        "Authors": [
            "Hongjin Su",
            "Howard Yen",
            "Mengzhou Xia",
            "Weijia Shi",
            "Han-yu Wang",
            "Haisu Liu",
            "Quan Shi",
            "Zachary S. Siegel",
            "Michael Tang",
            "Ruoxi Sun",
            "Jinsung Yoon",
            "Sercan \u00d6. Ar\u0131k",
            "Danqi Chen",
            "Tao Yu"
        ],
        "Affiliations": [
            "The University of Hong Kong",
            "Princeton University",
            "Google"
        ],
        "Abstract": "Existing retrieval benchmarks primarily consist of information-seeking queries (e.g., aggregated questions from search engines) where keyword or semantic-based retrieval is usually sufficient. However, many complex real-world queries require in-depth reasoning to identify relevant documents that go beyond surface form matching. For example, finding documentation for a coding question requires understanding the logic and syntax of the functions involved. To better benchmark retrieval on such challenging queries, we introduce textbf{Bright}, the first text retrieval benchmark that requires textit{intensive reasoning} to retrieve relevant documents. Our dataset consists of 1,384 real-world queries spanning diverse domains, such as economics, psychology, mathematics, and coding. These queries are drawn from naturally occurring and carefully curated human data. Extensive evaluation reveals that even state-of-the-art retrieval models perform poorly on textbf{Bright}. The leading model on the MTEB leaderboard (SFR-Embedding-Mistral) achieves a score of 59.0 nDCG@10. We show that incorporating explicit reasoning about the query improves retrieval performance by up to 12.2 points. Moreover, incorporating retrieved documents from the top-performing retriever boosts question-answering performance. We believe that textbf{Bright} paves the way for future research on retrieval systems in more realistic and challenging settings."
    },
    "validation": {
        "ACCESSABILITY": 0.5714285714285714,
        "DIVERSITY": 0.0,
        "CONTENT": 0.5714285714285714,
        "EVALUATION": 0.3333333333333333,
        "AVERAGE": 0.5
    },
    "length_forcing": 0.9999999999999996,
    "cost": {
        "cost": 0.00572466,
        "input_tokens": 54995,
        "output_tokens": 719
    },
    "config": {
        "model_name": "google_gemma-3-27b-it",
        "few_shot": 0,
        "month": null,
        "year": "2025",
        "keywords": [
            ""
        ],
        "link": "https://arxiv.org/abs/2407.12883"
    },
    "ratio_filling": 1.0,
    "error": null
}