{
    "metadata": {
        "Name": "Multi2WOZ",
        "Subsets": [
            {
                "Name": "Arabic",
                "Language": "Arabic",
                "Volume": 29500.0,
                "Unit": "sentences"
            },
            {
                "Name": "Chinese",
                "Language": "Chinese",
                "Volume": 29500.0,
                "Unit": "sentences"
            },
            {
                "Name": "German",
                "Language": "German",
                "Volume": 29500.0,
                "Unit": "sentences"
            },
            {
                "Name": "Russian",
                "Language": "Russian",
                "Volume": 29500.0,
                "Unit": "sentences"
            }
        ],
        "Link": "https://github.com/umanlp/Multi2WOZ",
        "HF Link": "",
        "License": "MIT License",
        "Year": 2022,
        "Language": [
            "Arabic",
            "Chinese",
            "German",
            "Russian"
        ],
        "Domain": [
            "public datasets"
        ],
        "Form": "text",
        "Collection Style": [
            "machine annotation",
            "human annotation"
        ],
        "Description": "A multilingual, multi-domain task-oriented dialog (TOD) dataset in Arabic, Chinese, German, and Russian. It was created by translating and manually post-editing the 2,000 development and test dialogs from the English MultiWOZ 2.1 dataset, enabling reliable cross-lingual transfer evaluation.",
        "Volume": 118000.0,
        "Unit": "sentences",
        "Ethical Risks": "Low",
        "Provider": [
            "University of Mannheim"
        ],
        "Derived From": [
            "MultiWOZ 2.1"
        ],
        "Paper Title": "Multi2WOZ: A Robust Multilingual Dataset and Conversational Pretraining for Task-Oriented Dialog",
        "Paper Link": "https://arxiv.org/pdf/2205.10400",
        "Tokenized": false,
        "Host": "GitHub",
        "Access": "Free",
        "Cost": "",
        "Test Split": false,
        "Tasks": [
            "instruction tuning"
        ],
        "Venue Title": "arXiv",
        "Venue Type": "preprint",
        "Venue Name": "",
        "Authors": [
            "Chia-Chien Hung",
            "Anne Lauscher",
            "Ivan Vulic\u00b4",
            "Simone Paolo Ponzetto",
            "Goran Glavas\u02c7"
        ],
        "Affiliations": [
            "Data and Web Science Group, University of Mannheim, Germany",
            "MilaNLP, Bocconi University, Italy",
            "LTL, University of Cambridge, UK",
            "CAIDAS, University of Wu\u00a8rzburg, Germany"
        ],
        "Abstract": "Research on (multi-domain) task-oriented dialog (TOD) has predominantly focused on the English language, primarily due to the shortage of robust TOD datasets in other languages, preventing the systematic investigation of cross-lingual transfer for this crucial NLP application area. In this work, we introduce MULTI2WOZ, a new multilingual multi-domain TOD dataset, derived from the well-established English dataset MULTIWOZ, that spans four typologically diverse languages: Chinese, German, Arabic, and Russian. In contrast to concurrent efforts (Ding et al., 2021; Zuo et al., 2021), MULTI2WOZ contains gold-standard dialogs in target languages that are directly comparable with development and test portions of the English dataset, enabling reliable and comparative estimates of cross-lingual transfer performance for TOD. We then introduce a new framework for multilingual conversational specialization of pretrained language models (PrLMs) that aims to facilitate cross-lingual transfer for arbitrary downstream TOD tasks. Using such conversational PrLMs specialized for concrete target languages, we systematically benchmark a number of zero-shot and few-shot crosslingual transfer approaches on two standard TOD tasks: Dialog State Tracking and Response Retrieval. Our experiments show that, in most setups, the best performance entails the combination of (i) conversational specialization in the target language and (ii) few-shot transfer for the concrete TOD task. Most importantly, we show that our conversational specialization in the target language allows for an exceptionally sample-efficient few-shot transfer for downstream TOD tasks.",
        "annotations_from_paper": {
            "Name": 1,
            "Subsets": 1,
            "Link": 1,
            "HF Link": 0,
            "License": 0,
            "Year": 1,
            "Language": 1,
            "Domain": 1,
            "Form": 1,
            "Collection Style": 1,
            "Description": 1,
            "Volume": 1,
            "Unit": 1,
            "Ethical Risks": 1,
            "Provider": 1,
            "Derived From": 1,
            "Paper Title": 1,
            "Paper Link": 1,
            "Tokenized": 1,
            "Host": 1,
            "Access": 1,
            "Cost": 1,
            "Test Split": 1,
            "Tasks": 1,
            "Venue Title": 1,
            "Venue Type": 1,
            "Venue Name": 1,
            "Authors": 1,
            "Affiliations": 1,
            "Abstract": 1
        }
    },
    "validation": {
        "DIVERSITY": 1.0,
        "ACCESSABILITY": 1.0,
        "CONTENT": 1.0,
        "EVALUATION": 1.0,
        "AVERAGE": 1.0
    },
    "length_forcing": 0.9999999999999999,
    "cost": {
        "cost": 0,
        "input_tokens": 0,
        "output_tokens": 0
    },
    "config": {
        "model_name": "human",
        "few_shot": 0,
        "month": null,
        "year": "2022",
        "keywords": [
            ""
        ],
        "link": "https://arxiv.org/abs/2205.10400"
    },
    "ratio_filling": 1.0,
    "error": null
}