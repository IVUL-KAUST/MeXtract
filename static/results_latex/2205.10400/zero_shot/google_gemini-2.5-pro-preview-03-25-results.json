{
    "metadata": {
        "Name": "Multi\u00b2WOZ",
        "Subsets": [
            {
                "Name": "German",
                "Volume": 2000,
                "Unit": "documents",
                "Language": "German"
            },
            {
                "Name": "Arabic",
                "Volume": 2000,
                "Unit": "documents",
                "Language": "Arabic"
            },
            {
                "Name": "Chinese",
                "Volume": 2000,
                "Unit": "documents",
                "Language": "Chinese"
            },
            {
                "Name": "Russian",
                "Volume": 2000,
                "Unit": "documents",
                "Language": "Russian"
            }
        ],
        "Link": "https://github.com/umanlp/Multi2WOZ",
        "HF Link": "",
        "License": "unknown",
        "Year": 2022,
        "Language": [
            "Arabic",
            "Chinese",
            "German",
            "Russian",
            "English"
        ],
        "Domain": [
            "public datasets"
        ],
        "Form": "text",
        "Collection Style": [
            "machine annotation",
            "human annotation"
        ],
        "Description": "A multilingual, multi-domain task-oriented dialog (TOD) dataset derived from the English MultiWOZ dataset. It spans four languages: Chinese, German, Arabic, and Russian. The dataset was created through automatic translation followed by manual post-editing to ensure high quality, enabling reliable and comparative evaluation of cross-lingual transfer performance for TOD tasks.",
        "Volume": 2000.0,
        "Unit": "documents",
        "Ethical Risks": "Medium",
        "Provider": [
            "University of Mannheim",
            "Bocconi University",
            "University of Cambridge",
            "University of W\u00fcrzburg"
        ],
        "Derived From": [
            "MultiWOZ 2.1"
        ],
        "Paper Title": "Multi\u00b2WOZ: A Robust Multilingual Dataset and Conversational Pretraining for Task-Oriented Dialog",
        "Paper Link": "https://aclanthology.org/2022.acl-long.55",
        "Tokenized": false,
        "Host": "GitHub",
        "Access": "Free",
        "Cost": "",
        "Test Split": true,
        "Tasks": [
            "information retrieval",
            "other"
        ],
        "Venue Title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
        "Venue Type": "conference",
        "Venue Name": "60th Annual Meeting of the Association for Computational Linguistics",
        "Authors": [
            "Chia-Chien Hung",
            "Anne Lauscher",
            "Ivan Vuli\u0107",
            "Simone Paolo Ponzetto",
            "Goran Glava\u0161"
        ],
        "Affiliations": [
            "Data and Web Science Group, University of Mannheim, Germany",
            "MilaNLP, Bocconi University, Italy",
            "LTL, University of Cambridge, UK",
            "CAIDAS, University of W\u00fcrzburg, Germany"
        ],
        "Abstract": "Research on (multi-domain) task-oriented dialog (TOD) has predominantly focused on the English language, primarily due to the shortage of robust TOD datasets in other languages, preventing the systematic investigation of cross-lingual transfer for this crucial NLP application area. In this work, we introduce Multi\u00b2WOZ, a new multilingual multi-domain TOD dataset, derived from the well-established English dataset MultiWOZ, that spans four typologically diverse languages: Chinese, German, Arabic, and Russian. In contrast to concurrent efforts, Multi\u00b2WOZ contains gold-standard dialogs in target languages that are directly comparable with development and test portions of the English dataset, enabling reliable and comparative estimates of cross-lingual transfer performance for TOD. We then introduce a new framework for multilingual conversational specialization of pretrained language models (PrLMs) that aims to facilitate cross-lingual transfer for arbitrary downstream TOD tasks. Using such conversational PrLMs specialized for concrete target languages, we systematically benchmark a number of zero-shot and few-shot cross-lingual transfer approaches on two standard TOD tasks: Dialog State Tracking and Response Retrieval. Our experiments show that, in most setups, the best performance entails the combination of (i) conversational specialization in the target language and (ii) few-shot transfer for the concrete TOD task. Most importantly, we show that our conversational specialization in the target language allows for an exceptionally sample-efficient few-shot transfer for downstream TOD tasks."
    },
    "validation": {
        "DIVERSITY": 0.5,
        "ACCESSABILITY": 0.7142857142857143,
        "CONTENT": 0.5714285714285714,
        "EVALUATION": 0.3333333333333333,
        "AVERAGE": 0.5789473684210527
    },
    "length_forcing": 0.9999999999999999,
    "cost": {
        "cost": 0.0794,
        "input_tokens": 20792,
        "output_tokens": 2120
    },
    "config": {
        "model_name": "google_gemini-2.5-pro-preview-03-25",
        "few_shot": 0,
        "month": null,
        "year": "2022",
        "keywords": [
            ""
        ],
        "link": "https://arxiv.org/abs/2205.10400"
    },
    "ratio_filling": 1.0,
    "error": null
}