{
    "metadata": {
        "Name": "Multi2WOZ",
        "Subsets": [
            {
                "Name": "Arabic",
                "Volume": 29492,
                "Unit": "sentences",
                "Language": "Arabic"
            },
            {
                "Name": "Chinese",
                "Volume": 29492,
                "Unit": "sentences",
                "Language": "Chinese"
            },
            {
                "Name": "German",
                "Volume": 29492,
                "Unit": "sentences",
                "Language": "German"
            },
            {
                "Name": "Russian",
                "Volume": 29492,
                "Unit": "sentences",
                "Language": "Russian"
            }
        ],
        "Link": "https://github.com/umanlp/Multi2WOZ",
        "HF Link": "",
        "License": "unknown",
        "Year": 2022,
        "Language": [
            "English",
            "Arabic",
            "Chinese",
            "German",
            "Russian"
        ],
        "Domain": [
            "public datasets"
        ],
        "Form": "text",
        "Collection Style": [
            "machine annotation",
            "human annotation"
        ],
        "Description": "A multilingual, multi-domain task-oriented dialog (TOD) dataset in Arabic, Chinese, German, and Russian. It was created by translating and manually post-editing the 2,000 development and test dialogs from the English MultiWOZ 2.1 dataset, enabling reliable cross-lingual transfer evaluation.",
        "Volume": 29492.0,
        "Unit": "sentences",
        "Ethical Risks": "Low",
        "Provider": [
            "University of Mannheim",
            "Bocconi University",
            "University of Cambridge",
            "University of Wu\u00a8rzburg"
        ],
        "Derived From": [
            "MultiWOZ 2.1"
        ],
        "Paper Title": "Multi2WOZ: A Robust Multilingual Dataset and Conversational Pretraining for Task-Oriented Dialog",
        "Paper Link": "https://arxiv.org/abs/2205.10400",
        "Tokenized": false,
        "Host": "GitHub",
        "Access": "Free",
        "Cost": "",
        "Test Split": true,
        "Tasks": [
            "information retrieval",
            "other"
        ],
        "Venue Title": "arXiv",
        "Venue Type": "preprint",
        "Venue Name": "arXiv",
        "Authors": [
            "Chia-Chien Hung",
            "Anne Lauscher",
            "Ivan Vulic\u00b4",
            "Simone Paolo Ponzetto",
            "Goran Glavas\u02c7"
        ],
        "Affiliations": [
            "Data and Web Science Group, University of Mannheim, Germany",
            "MilaNLP, Bocconi University, Italy",
            "LTL, University of Cambridge, UK",
            "CAIDAS, University of Wu\u00a8rzburg, Germany"
        ],
        "Abstract": "Research on (multi-domain) task-oriented dialog (TOD) has predominantly focused on the English language, primarily due to the shortage of robust TOD datasets in other languages, preventing the systematic investigation of cross-lingual transfer for this crucial NLP application area. In this work, we introduce MULTI2WOZ, a new multilingual multi-domain TOD dataset, derived from the well-established English dataset MULTIWOZ, that spans four typologically diverse languages: Chinese, German, Arabic, and Russian. In contrast to concurrent efforts (Ding et al., 2021; Zuo et al., 2021), MULTI2WOZ contains gold-standard dialogs in target languages that are directly comparable with development and test portions of the English dataset, enabling reliable and comparative estimates of cross-lingual transfer performance for TOD. We then introduce a new framework for multilingual conversational specialization of pretrained language models (PrLMs) that aims to facilitate cross-lingual transfer for arbitrary downstream TOD tasks. Using such conversational PrLMs specialized for concrete target languages, we systematically benchmark a number of zero-shot and few-shot crosslingual transfer approaches on two standard TOD tasks: Dialog State Tracking and Response Retrieval. Our experiments show that, in most setups, the best performance entails the combination of (i) conversational specialization in the target language and (ii) few-shot transfer for the concrete TOD task. Most importantly, we show that our conversational specialization in the target language allows for an exceptionally sample-efficient few-shot transfer for downstream TOD tasks."
    },
    "validation": {},
    "cost": {
        "cost": 0.07570125,
        "input_tokens": 20708,
        "output_tokens": 2014
    },
    "config": {
        "model_name": "google_gemini-2.5-pro",
        "few_shot": 0,
        "month": null,
        "year": null,
        "keywords": [
            ""
        ],
        "link": "https://arxiv.org/pdf/2205.10400"
    },
    "ratio_filling": 1.0,
    "error": null
}