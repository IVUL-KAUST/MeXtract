{
    "metadata": {
        "Name": "SciQ",
        "Link": "https://huggingface.co/datasets/allenai/sciq",
        "HF Link": "https://huggingface.co/datasets/allenai/sciq",
        "License": "CC BY-NC 3.0",
        "Year": 2017,
        "Language": "en",
        "Domain": [
            "books",
            "web pages"
        ],
        "Form": "text",
        "Collection Style": [
            "human annotation"
        ],
        "Description": "SciQ is a dataset containing 13,679 multiple-choice science exam questions. It was created using a novel crowdsourcing method that leverages a large corpus of domain-specific text (science textbooks) and a model trained on existing questions to suggest document selections and answer distractors, aiding human workers in the question generation process.",
        "Volume": 13679.0,
        "Unit": "sentences",
        "Ethical Risks": "Low",
        "Provider": [
            "Allen Institute for Artificial Intelligence"
        ],
        "Derived From": [],
        "Paper Title": "Crowdsourcing Multiple Choice Science Questions",
        "Paper Link": "https://arxiv.org/pdf/1707.06209",
        "Tokenized": false,
        "Host": "HuggingFace",
        "Access": "Free",
        "Cost": "",
        "Test Split": true,
        "Tasks": [
            "multiple choice question answering",
            "question answering"
        ],
        "Venue Title": "arXiv",
        "Venue Type": "preprint",
        "Venue Name": "",
        "Authors": [
            "Johannes Welbl",
            "Nelson F. Liu",
            "Matt Gardner"
        ],
        "Affiliations": [
            "Allen Institute for Artificial Intelligence",
            "University of Washington",
            "University College London"
        ],
        "Abstract": "We present a novel method for obtaining high-quality, domain-targeted multiple choice questions from crowd workers. Generating these questions can be difficult without trading away originality, relevance or diversity in the answer options. Our method addresses these problems by leveraging a large corpus of domain-specific text and a small set of existing questions. It produces model suggestions for document selection and answer distractor choice which aid the human question generation process. With this method we have assembled SciQ, a dataset of 13.7K multiple choice science exam questions. We demonstrate that the method produces in-domain questions by providing an analysis of this new dataset and by showing that humans cannot distinguish the crowdsourced questions from original questions. When using SciQ as additional training data to existing questions, we observe accuracy improvements on real science exams.",
        "annotations_from_paper": {
            "Name": 1,
            "Link": 0,
            "HF Link": 0,
            "License": 0,
            "Year": 1,
            "Language": 1,
            "Domain": 1,
            "Form": 1,
            "Collection Style": 1,
            "Description": 1,
            "Volume": 1,
            "Unit": 1,
            "Ethical Risks": 1,
            "Provider": 1,
            "Derived From": 1,
            "Paper Title": 1,
            "Paper Link": 1,
            "Tokenized": 1,
            "Host": 0,
            "Access": 1,
            "Cost": 1,
            "Test Split": 1,
            "Tasks": 1,
            "Venue Title": 1,
            "Venue Type": 1,
            "Venue Name": 1,
            "Authors": 1,
            "Affiliations": 1,
            "Abstract": 1
        }
    },
    "validation": {
        "ACCESSABILITY": 1.0,
        "DIVERSITY": 1.0,
        "CONTENT": 1.0,
        "EVALUATION": 1.0,
        "AVERAGE": 1.0
    },
    "length_forcing": 0.9999999999999996,
    "cost": {
        "cost": 0,
        "input_tokens": 0,
        "output_tokens": 0
    },
    "config": {
        "model_name": "human",
        "few_shot": 0,
        "month": null,
        "year": "2017",
        "keywords": [
            ""
        ],
        "link": "https://arxiv.org/abs/1707.06209"
    },
    "ratio_filling": 1.0,
    "error": null
}