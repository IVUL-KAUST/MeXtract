{
    "metadata": {
        "Name": "Pensez-2k",
        "Link": "",
        "HF Link": "",
        "License": "unknown",
        "Year": 2025,
        "Language": "multilingual",
        "Domain": [
            "public datasets"
        ],
        "Form": "text",
        "Collection Style": [
            "manual curation",
            "LLM generated"
        ],
        "Description": "A small, high-quality, bilingual (English-French) dataset of 2,000 samples designed for supervised fine-tuning. It aims to enhance mathematical reasoning and French language proficiency in LLMs, curated from various public reasoning and conversational datasets.",
        "Volume": 1000.0,
        "Unit": "documents",
        "Ethical Risks": "Low",
        "Provider": [
            "Menlo Research",
            "Universit\u00e9 Grenoble Alpes"
        ],
        "Derived From": [
            "LIMO",
            "Dolphin-r1",
            "OpenR1-Math-220k",
            "s1K-1.1",
            "MagpieLLama-3-70B",
            "Qwen 2.5-72B",
            "Tulu-3"
        ],
        "Paper Title": "Pensez: Less Data, Better Reasoning \u2013 Rethinking French LLM",
        "Paper Link": "https://arxiv.org/abs/2505.13661",
        "Tokenized": false,
        "Host": "other",
        "Access": "Free",
        "Cost": "",
        "Test Split": false,
        "Tasks": [
            "instruction tuning",
            "image captioning",
            "question answering"
        ],
        "Venue Title": "arXiv",
        "Venue Type": "preprint",
        "Venue Name": "arXiv",
        "Authors": [
            "Huy Hoang Ha"
        ],
        "Affiliations": [
            "Menlo Research",
            "Universit\u00e9 Grenoble Alpes"
        ],
        "Abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in various natural language processing tasks. However, achieving strong performance in specialized domains like mathematical reasoning and non-English languages often requires extensive training on massive datasets. This paper investigates a contrasting approach: strategic fine-tuning on a small, high-quality, bilingual (English-French) dataset to enhance both the reasoning capabilities and French language proficiency of a large language model. Rather than relying on scale, we explore the hypothesis that targeted data curation and optimized training can achieve competitive, or even superior, performance. We demonstrate, through targeted supervised fine-tuning (SFT) on only 2,000 carefully selected samples, significant improvements in mathematical reasoning. Specifically, Pensez 7B exhibits an increase in accuracy of the base model up to 20 points on the AIME25 and a 12 point increase on a French MATH level 5 benchmark. These results challenge the prevailing assumption that massive datasets are a prerequisite for strong reasoning performance in LLMs, highlighting the potential of strategic data curation and optimized fine-tuning for enhancing both specialized skills and multilingual capabilities. Our findings have implications for the efficient development of high-performing, multilingual LLMs, especially in resource-constrained scenarios."
    },
    "validation": {},
    "cost": {
        "cost": 0.07195625,
        "input_tokens": 23595,
        "output_tokens": 1693
    },
    "config": {
        "model_name": "google_gemini-2.5-pro",
        "few_shot": 0,
        "month": null,
        "year": null,
        "keywords": [
            ""
        ],
        "link": "https://arxiv.org/pdf/2503.13661"
    },
    "ratio_filling": 1.0,
    "error": null
}