{
    "metadata": {
        "Name": "STAIR Captions",
        "Link": "http://captions.stair.center/",
        "HF Link": "https://huggingface.co/datasets/shunk031/STAIR-Captions",
        "License": "CC BY 4.0",
        "Year": 2017,
        "Language": "jp",
        "Domain": [
            "captions",
            "public datasets"
        ],
        "Form": "text",
        "Collection Style": [
            "human annotation"
        ],
        "Description": "A large-scale image caption dataset in Japanese, based on the COCO dataset. It contains 820,310 Japanese captions for 164,062 images, collected via crowdsourcing.",
        "Volume": 820310.0,
        "Unit": "sentences",
        "Ethical Risks": "Low",
        "Provider": [
            "The University of Tokyo",
            "National Institute of Informatics"
        ],
        "Derived From": [
            "MS COCO"
        ],
        "Paper Title": "STAIR Captions: Constructing a Large-Scale Japanese Image Caption Dataset",
        "Paper Link": "https://arxiv.org/pdf/1705.00823",
        "Script": "mixed",
        "Tokenized": false,
        "Host": "other",
        "Access": "Free",
        "Cost": "",
        "Test Split": false,
        "Tasks": [
            "image captioning"
        ],
        "Venue Title": "arXiv",
        "Venue Type": "preprint",
        "Venue Name": "",
        "Authors": [
            "Yuya Yoshikawa",
            "Yutaro Shigeto",
            "Akikazu Takeuchi"
        ],
        "Affiliations": [
            "The University of Tokyo",
            "National Institute of Informatics"
        ],
        "Abstract": "We introduce a new, large-scale image caption dataset in Japanese, which we call STAIR Captions. While several caption datasets have been developed for English, there are currently no large-scale caption datasets for other languages such as Japanese. Our new dataset is based on the COCO dataset, which is a standard benchmark for image captioning, and it contains 820,310 Japanese captions for 164,062 images. We collected the captions using crowdsourcing. We describe the details of our dataset and the collection process. We also report the performance of baseline image captioning models on our dataset. The dataset is freely available at http://captions.stair.center/.",
        "annotations_from_paper": {
            "Name": 1,
            "Link": 1,
            "HF Link": 0,
            "License": 0,
            "Year": 1,
            "Language": 1,
            "Domain": 1,
            "Form": 1,
            "Collection Style": 1,
            "Description": 1,
            "Volume": 1,
            "Unit": 1,
            "Ethical Risks": 1,
            "Provider": 1,
            "Derived From": 1,
            "Paper Title": 1,
            "Paper Link": 1,
            "Script": 1,
            "Tokenized": 1,
            "Host": 1,
            "Access": 1,
            "Cost": 1,
            "Test Split": 1,
            "Tasks": 1,
            "Venue Title": 1,
            "Venue Type": 1,
            "Venue Name": 1,
            "Authors": 1,
            "Affiliations": 1,
            "Abstract": 1
        }
    },
    "validation": {
        "ACCESSABILITY": 1.0,
        "DIVERSITY": 1.0,
        "CONTENT": 1.0,
        "EVALUATION": 1.0,
        "AVERAGE": 1.0
    },
    "length_forcing": 0.9999999999999999,
    "cost": {
        "cost": 0,
        "input_tokens": 0,
        "output_tokens": 0
    },
    "config": {
        "model_name": "human",
        "few_shot": 0,
        "month": null,
        "year": "2017",
        "keywords": [
            ""
        ],
        "link": "https://arxiv.org/abs/1705.00823"
    },
    "ratio_filling": 1.0,
    "error": null
}