{
    "metadata": {
        "Name": "STAIR Captions",
        "Link": "https://huggingface.co/datasets/stair-lab/STAIR-captions",
        "HF Link": "https://huggingface.co/datasets/stair-lab/STAIR-captions",
        "License": "CC BY 4.0",
        "Year": 2017,
        "Language": "jp",
        "Domain": [
            "captions"
        ],
        "Form": "text",
        "Collection Style": [
            "human annotation"
        ],
        "Description": "A large-scale image caption dataset in Japanese, based on the COCO dataset. It contains 820,310 Japanese captions for 164,062 images, collected via crowdsourcing.",
        "Volume": 820310.0,
        "Unit": "sentences",
        "Ethical Risks": "Low",
        "Provider": [
            "The University of Tokyo",
            "National Institute of Informatics"
        ],
        "Derived From": [
            "MS COCO"
        ],
        "Paper Title": "STAIR Captions: Constructing a Large-Scale Japanese Image Caption Dataset",
        "Paper Link": "https://aclanthology.org/P17-2082",
        "Script": "mixed",
        "Tokenized": false,
        "Host": "HuggingFace",
        "Access": "Free",
        "Cost": "",
        "Test Split": false,
        "Tasks": [
            "text generation",
            "information retrieval"
        ],
        "Venue Title": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
        "Venue Type": "conference",
        "Venue Name": "Association for Computational Linguistics",
        "Authors": [
            "Yuya Yoshikawa",
            "Yutaro Shigeto",
            "Akikazu Takeuchi"
        ],
        "Affiliations": [
            "The University of Tokyo",
            "National Institute of Informatics"
        ],
        "Abstract": "We introduce a new, large-scale image caption dataset in Japanese, which we call STAIR Captions. While several caption datasets have been developed for English, there are currently no large-scale caption datasets for other languages such as Japanese. Our new dataset is based on the COCO dataset, which is a standard benchmark for image captioning, and it contains 820,310 Japanese captions for 164,062 images. We collected the captions using crowdsourcing. We describe the details of our dataset and the collection process. We also report the performance of baseline image captioning models on our dataset. The dataset is freely available at http://captions.stair.center/."
    },
    "validation": {},
    "cost": {
        "cost": 0.04531375,
        "input_tokens": 3353,
        "output_tokens": 1491
    },
    "config": {
        "model_name": "google_gemini-2.5-pro",
        "few_shot": 0,
        "month": null,
        "year": null,
        "keywords": [
            ""
        ],
        "link": "https://arxiv.org/pdf/1705.00823"
    },
    "ratio_filling": 1.0,
    "error": null
}