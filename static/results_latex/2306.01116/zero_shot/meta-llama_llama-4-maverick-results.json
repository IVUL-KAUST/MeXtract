{
    "metadata": {
        "Name": "RefinedWeb",
        "Link": "https://huggingface.co/datasets/tiiuae/falcon-refinedweb",
        "HF Link": "https://huggingface.co/datasets/tiiuae/falcon-refinedweb",
        "License": "ODbl-1.0",
        "Year": 2023,
        "Language": "en",
        "Domain": [
            "web pages"
        ],
        "Form": "text",
        "Collection Style": [
            "crawling"
        ],
        "Description": "A large-scale dataset for pretraining large language models",
        "Volume": 5000.0,
        "Unit": "tokens",
        "Ethical Risks": "Medium",
        "Provider": [
            "Technology Innovation Institute"
        ],
        "Derived From": [
            "CommonCrawl"
        ],
        "Paper Title": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only",
        "Paper Link": "https://arxiv.org/abs/2306.01116",
        "Tokenized": false,
        "Host": "HuggingFace",
        "Access": "Free",
        "Cost": "",
        "Test Split": false,
        "Tasks": [
            "language modeling"
        ],
        "Venue Title": "ICML",
        "Venue Type": "conference",
        "Venue Name": "International Conference on Machine Learning",
        "Authors": [
            "Guilherme Penedo",
            "Quentin Malartic",
            "Daniel Hesslow",
            "Ruxandra Cojocaru",
            "Alessandro Cappelli",
            "Hamza Alobeidli",
            "Baptiste Pannier",
            "Ebtesam Almazrouei",
            "Julien Launay"
        ],
        "Affiliations": [
            "LightOn",
            "Technology Innovation Institute",
            "LPENS, \u00c9cole normale sup\u00e9rieure"
        ],
        "Abstract": "Large language models are commonly trained on a mixture of filtered web data and curated 'high-quality' corpora. This paper challenges the belief that curation is necessary by showing that properly filtered and deduplicated web data alone can lead to powerful models, even outperforming models trained on The Pile. The authors introduce RefinedWeb, a high-quality five trillion tokens web-only English pretraining dataset, and release a 600 billion tokens extract of it. They also release 1.3 and 7.5 billion parameters language models trained on RefinedWeb, demonstrating its effectiveness."
    },
    "validation": {
        "ACCESSABILITY": 0.8571428571428571,
        "DIVERSITY": 1.0,
        "CONTENT": 0.8571428571428571,
        "EVALUATION": 1.0,
        "AVERAGE": 0.8888888888888888
    },
    "length_forcing": 0.9999999999999996,
    "cost": {
        "cost": 0.0050875,
        "input_tokens": 33198,
        "output_tokens": 511
    },
    "config": {
        "model_name": "meta-llama_llama-4-maverick",
        "few_shot": 0,
        "month": null,
        "year": "2023",
        "keywords": [
            ""
        ],
        "link": "https://arxiv.org/abs/2306.01116"
    },
    "ratio_filling": 1.0,
    "error": null
}