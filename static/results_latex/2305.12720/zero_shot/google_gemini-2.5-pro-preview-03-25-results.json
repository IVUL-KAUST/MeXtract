{
    "metadata": {
        "Name": "llm-japanese-dataset v0",
        "Link": "https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset",
        "HF Link": "https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset",
        "License": "custom",
        "Year": 2023,
        "Language": "jp",
        "Domain": [
            "public datasets",
            "captions",
            "wikipedia",
            "books",
            "news articles"
        ],
        "Form": "text",
        "Collection Style": [
            "manual curation",
            "crawling"
        ],
        "Description": "A Japanese chat dataset with approximately 8.4 million records, created for tuning Large Language Models (LLMs). It is composed of various tasks, including translation, knowledge-based Q&A, and summarization, by processing and combining multiple existing Japanese language resources.",
        "Volume": 8393726.0,
        "Unit": "sentences",
        "Ethical Risks": "Low",
        "Provider": [
            "The University of Tokyo"
        ],
        "Derived From": [
            "Coursera Corpus Mining and Multistage Fine-Tuning for Improving Lectures Translation",
            "ParaNatCom",
            "Tab-delimited Bilingual Sentence Pairs",
            "Asian Language Treebank (ALT) Project",
            "Tanaka Corpus",
            "Japanese-English Subtitle Corpus",
            "Japanese WordNet",
            "Easy Japanese Corpus",
            "Wikipedia",
            "AIO (AI King) Official Distribution Dataset Version 2.0",
            "Japanese Movie Recommendation Dialogue (JMRD)",
            "JCommonsenseQA",
            "Aozora Paperback",
            "Wikinews",
            "JSQuAD",
            "Alpaca",
            "Dolly"
        ],
        "Paper Title": "llm-japanese-dataset v0: Construction of Japanese Chat Dataset for Large Language Models and its Methodology",
        "Paper Link": "https://arxiv.org/abs/2305.13122",
        "Script": "mixed",
        "Tokenized": false,
        "Host": "HuggingFace",
        "Access": "Free",
        "Cost": "",
        "Test Split": false,
        "Tasks": [
            "instruction tuning",
            "machine translation",
            "question answering",
            "summarization",
            "text generation"
        ],
        "Venue Title": "Companion volume of the proceedings of The 16th International Natural Language Generation Conference",
        "Venue Type": "conference",
        "Venue Name": "The 16th International Natural Language Generation Conference (INLG 2023)",
        "Authors": [
            "Masanori HIRANO",
            "Masahiro SUZUKI",
            "Hiroki SAKAJI"
        ],
        "Affiliations": [
            "The University of Tokyo"
        ],
        "Abstract": "This study constructed a Japanese chat dataset for tuning large language models (LLMs), which consist of about 8.4 million records. Recently, LLMs have been developed and gaining popularity. However, high-performing LLMs are usually mainly for English. There are two ways to support languages other than English by those LLMs: constructing LLMs from scratch or tuning existing models. However, in both ways, datasets are necessary parts. In this study, we focused on supporting Japanese in those LLMs and making a dataset for training or tuning LLMs in Japanese. The dataset we constructed consisted of various tasks, such as translation and knowledge tasks. In our experiment, we tuned an existing LLM using our dataset and evaluated the performance qualitatively. The results suggest that our dataset is possibly beneficial for LLMs. However, we also revealed some difficulties in constructing LLMs in languages other than English."
    },
    "validation": {
        "ACCESSABILITY": 0.8571428571428571,
        "DIVERSITY": 0.0,
        "CONTENT": 0.75,
        "EVALUATION": 0.3333333333333333,
        "AVERAGE": 0.6842105263157895
    },
    "length_forcing": 0.9999999999999999,
    "cost": {
        "cost": 0.056865,
        "input_tokens": 11196,
        "output_tokens": 1821
    },
    "config": {
        "model_name": "google_gemini-2.5-pro-preview-03-25",
        "few_shot": 0,
        "month": null,
        "year": "2023",
        "keywords": [
            ""
        ],
        "link": "https://arxiv.org/abs/2305.12720"
    },
    "ratio_filling": 1.0,
    "error": null
}