{
    "metadata": {
        "Name": "SQuAD",
        "Link": "https://stanford-qa.com",
        "HF Link": "",
        "License": "unknown",
        "Year": 2016,
        "Language": "en",
        "Domain": [
            "wikipedia"
        ],
        "Form": "text",
        "Collection Style": [
            "human annotation"
        ],
        "Description": "The Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage.",
        "Volume": 100000.0,
        "Unit": "sentences",
        "Ethical Risks": "Low",
        "Provider": [
            "Stanford University"
        ],
        "Derived From": [],
        "Paper Title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
        "Paper Link": "https://arxiv.org/abs/1606.05250",
        "Tokenized": false,
        "Host": "other",
        "Access": "Free",
        "Cost": "",
        "Test Split": true,
        "Tasks": [
            "question answering"
        ],
        "Venue Title": "",
        "Venue Type": "conference",
        "Venue Name": "",
        "Authors": [
            "Pranav Rajpurkar",
            "Jian Zhang",
            "Konstantin Lopyrev",
            "Percy Liang"
        ],
        "Affiliations": [
            "Stanford University"
        ],
        "Abstract": "In meteorology, precipitation is any product of the condensation of atmospheric water vapor that falls under gravity. The main forms of precipitation include drizzle, rain, sleet, snow, graupel and hail... Precipitation forms as smaller droplets coalesce via collision with other rain drops or ice crystals within a cloud. Short, intense periods of rain in scattered locations are called 'showers'. We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at https://stanford-qa.com."
    },
    "validation": {
        "ACCESSABILITY": 0.42857142857142855,
        "DIVERSITY": 1.0,
        "CONTENT": 0.8571428571428571,
        "EVALUATION": 1.0,
        "AVERAGE": 0.7222222222222222
    },
    "length_forcing": 0.9999999999999996,
    "cost": {
        "cost": 0.00407636,
        "input_tokens": 12724,
        "output_tokens": 537
    },
    "config": {
        "model_name": "deepseek_deepseek-chat-v3-0324",
        "few_shot": 0,
        "month": null,
        "year": "2016",
        "keywords": [
            ""
        ],
        "link": "https://arxiv.org/abs/1606.05250"
    },
    "ratio_filling": 1.0,
    "error": null
}