{
    "metadata": {
        "Name": "MMLU-Pro",
        "Link": "https://huggingface.co/spaces/TIGER-Lab/MMLU-Pro",
        "HF Link": "https://huggingface.co/spaces/TIGER-Lab/MMLU-Pro",
        "License": "MIT License",
        "Year": 2024,
        "Language": "en",
        "Domain": [
            "captions",
            "news articles",
            "commentary",
            "LLM",
            "reviews",
            "TV Channels",
            "commentary",
            "news articles",
            "other",
            "captions",
            "other"
        ],
        "Form": "text",
        "Collection Style": [
            "crawling",
            "human annotation",
            "machine annotation"
        ],
        "Description": "MMLU-Pro is an enhanced dataset designed to extend the mostly knowledge-driven MMLU benchmark by integrating more challenging, reasoning-focused questions and expanding the choice set from four to ten options. It includes over 12,000 questions across 14 diverse domains.",
        "Volume": 12032.0,
        "Unit": "tokens",
        "Ethical Risks": "Low",
        "Provider": [
            "University of Waterloo",
            "University of Toronto",
            "Carnegie Mellon University"
        ],
        "Derived From": [
            "MMLU",
            "STEM Website",
            "TheoremQA",
            "SciBench"
        ],
        "Paper Title": "MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark",
        "Paper Link": "https://arxiv.org/abs/2404.14219",
        "Tokenized": false,
        "Host": "HuggingFace",
        "Access": "Free",
        "Cost": "",
        "Test Split": true,
        "Tasks": [
            "language modeling",
            "commonsense reasoning",
            "information retrieval"
        ],
        "Venue Title": "arXiv preprint arXiv:2404.14219",
        "Venue Type": "preprint",
        "Venue Name": "",
        "Authors": [
            "Yubo Wang",
            "Xueguang Ma",
            "Ge Zhang",
            "Yuansheng Ni",
            "Abhranil Chandra",
            "Shiguang Guo",
            "Weiming Ren",
            "Aaran Arulraj",
            "Xuan He",
            "Ziyan Jiang",
            "Tianle Li",
            "Max Ku",
            "Kai Wang",
            "Alex Zhuang",
            "Rongqi Fan",
            "Xiang Yue",
            "Wenhu Chen"
        ],
        "Affiliations": [
            "University of Waterloo",
            "University of Toronto",
            "Carnegie Mellon University"
        ],
        "Abstract": "In the age of large-scale language models, benchmarks like the Massive Multitask Language Understanding (MMLU) have been pivotal in pushing the boundaries of what AI can achieve in language comprehension and reasoning across diverse domains. However, as models continue to improve, their performance on these benchmarks has begun to plateau, making it increasingly difficult to discern differences in model capabilities. This paper introduces MMLU-Pro, an enhanced dataset designed to extend the mostly knowledge-driven MMLU benchmark by integrating more challenging, reasoning-focused questions and expanding the choice set from four to ten options. Additionally, MMLU-Pro eliminates the trivial and noisy questions in MMLU. Our experimental results show that MMLU-Pro not only raises the challenge, causing a significant drop in accuracy by 16% to 33% compared to MMLU but also demonstrates greater stability under varying prompts."
    },
    "validation": {
        "ACCESSABILITY": 0.7142857142857143,
        "DIVERSITY": 1.0,
        "CONTENT": 0.5714285714285714,
        "EVALUATION": 0.6666666666666666,
        "AVERAGE": 0.6666666666666666
    },
    "length_forcing": 0.9999999999999996,
    "cost": {
        "cost": 0.00516255,
        "input_tokens": 35363,
        "output_tokens": 714
    },
    "config": {
        "model_name": "qwen_qwen-2.5-72b-instruct",
        "few_shot": 0,
        "month": null,
        "year": "2024",
        "keywords": [
            ""
        ],
        "link": "https://arxiv.org/abs/2406.01574"
    },
    "ratio_filling": 1.0,
    "error": null
}