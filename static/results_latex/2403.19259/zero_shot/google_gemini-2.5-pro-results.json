{
    "metadata": {
        "Name": "J-CRe3",
        "Link": "https://github.com/riken-grp/J-CRe3",
        "HF Link": "",
        "License": "unknown",
        "Year": 2024,
        "Language": "jp",
        "Domain": [
            "other"
        ],
        "Form": "text",
        "Collection Style": [
            "human annotation",
            "machine annotation",
            "manual curation"
        ],
        "Description": "A Japanese multimodal dataset containing egocentric video and dialogue audio of real-world conversations between a master and an assistant robot at home. It is annotated with cross-modal tags between phrases in utterances and object bounding boxes in video frames. These tags include indirect reference relations, such as predicate-argument structures and bridging references, as well as direct reference relations.",
        "Volume": 2652.0,
        "Unit": "sentences",
        "Ethical Risks": "Low",
        "Provider": [
            "Kyoto University",
            "RIKEN",
            "Tokyo University of Science",
            "Nara Institute of Science and Technology",
            "National Institute of Informatics"
        ],
        "Derived From": [],
        "Paper Title": "J-CRe3: A Japanese Conversation Dataset for Real-world Reference Resolution",
        "Paper Link": "https://arxiv.org/abs/2403.19259",
        "Script": "mixed",
        "Tokenized": true,
        "Host": "GitHub",
        "Access": "Free",
        "Cost": "",
        "Test Split": true,
        "Tasks": [
            "other"
        ],
        "Venue Title": "arXiv",
        "Venue Type": "preprint",
        "Venue Name": "arXiv",
        "Authors": [
            "Nobuhiro Ueda",
            "Hideko Habe",
            "Yoko Matsui",
            "Akishige Yuguchi",
            "Seiya Kawano",
            "Yasutomo Kawanishi",
            "Sadao Kurohashi",
            "Koichiro Yoshino"
        ],
        "Affiliations": [
            "Kyoto University",
            "Guardian Robot Project, R-IH, RIKEN",
            "Tokyo University of Science",
            "Nara Institute of Science and Technology",
            "National Institute of Informatics"
        ],
        "Abstract": "Understanding expressions that refer to the physical world is crucial for such human-assisting systems in the realworld,asrobotsthatmustperformactionsthatareexpectedbyusers. Inreal-worldreferenceresolution,a system must ground the verbal information that appears in user interactions to the visual information observed in egocentric views. To this end, we propose a multimodal reference resolution task and construct a Japanese ConversationdatasetforReal-worldReferenceResolution(J-CRe3). Ourdatasetcontainsegocentricvideoand dialogueaudioofreal-worldconversationsbetweentwopeopleactingasamasterandanassistantrobotathome. Thedatasetisannotatedwithcrossmodaltagsbetweenphrasesintheutterancesandtheobjectboundingboxes inthevideoframes. Thesetagsincludeindirectreferencerelations,suchaspredicate-argumentstructuresand bridgingreferencesaswellasdirectreferencerelations. Wealsoconstructedanexperimentalmodelandclarifiedthe challengesinmultimodalreferenceresolutiontasks."
    },
    "validation": {},
    "cost": {
        "cost": 0.06592875,
        "input_tokens": 17768,
        "output_tokens": 1765
    },
    "config": {
        "model_name": "google_gemini-2.5-pro",
        "few_shot": 0,
        "month": null,
        "year": null,
        "keywords": [
            ""
        ],
        "link": "https://arxiv.org/pdf/2403.19259"
    },
    "ratio_filling": 1.0,
    "error": null
}