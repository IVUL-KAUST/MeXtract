{
    "metadata": {
        "Name": "WinoGrande",
        "Link": "http://winogrande.allenai.org",
        "HF Link": "",
        "License": "unknown",
        "Year": 2020,
        "Language": "en",
        "Domain": [
            "social media",
            "news articles",
            "books"
        ],
        "Form": "text",
        "Collection Style": [
            "crawling",
            "manual curation"
        ],
        "Description": "A large-scale dataset of 44k WSC-inspired problems designed to be more challenging for AI models while maintaining human solvability.",
        "Volume": 44000.0,
        "Unit": "sentences",
        "Ethical Risks": "Low",
        "Provider": [
            "Allen Institute for Artificial Intelligence",
            "University of Washington"
        ],
        "Derived From": [
            "WSC",
            "PDP",
            "DPR",
            "KnowRef",
            "COPA",
            "Winogender"
        ],
        "Paper Title": "WinoGrande: An Adversarial Winograd Schema Challenge at Scale",
        "Paper Link": "https://github.com/allenai/winoGrande",
        "Tokenized": false,
        "Host": "GitHub",
        "Access": "Free",
        "Cost": "",
        "Test Split": true,
        "Tasks": [
            "commonsense reasoning",
            "irony detection",
            "commonsense reasoning"
        ],
        "Venue Title": "AAAI Conference on Artificial Intelligence",
        "Venue Type": "conference",
        "Venue Name": "AAAI",
        "Authors": [
            "Keisuke Sakaguchi",
            "Ronan Le Bras",
            "Chandra Bhagavatula",
            "Yejin Choi"
        ],
        "Affiliations": [
            "Allen Institute for Artificial Intelligence",
            "University of Washington"
        ],
        "Abstract": "The Winograd Schema Challenge (WSC), a benchmark for commonsense reasoning, is a set of 273 expert-crafted pronoun resolution problems originally designed to be unsolvable for statistical models that rely on selectional preferences or word associations. However, recent advances in neural language models have already reached around 90% accuracy on variants of WSC. This raises an important question whether these models have truly acquired robust commonsense capabilities or whether they rely on spurious biases in the datasets that lead to an overestimation of the true capabilities of machine commonsense. To investigate this question, we introduce WinoGrande, a large-scale dataset of 44k problems, inspired by the original WSC design, but adjusted to improve both the scale and the hardness of the dataset."
    },
    "validation": {
        "ACCESSABILITY": 0.5714285714285714,
        "DIVERSITY": 1.0,
        "CONTENT": 0.7142857142857143,
        "EVALUATION": 0.6666666666666666,
        "AVERAGE": 0.6666666666666666
    },
    "length_forcing": 0.9999999999999996,
    "cost": {
        "cost": 0.00263602,
        "input_tokens": 18186,
        "output_tokens": 529
    },
    "config": {
        "model_name": "qwen_qwen-2.5-72b-instruct",
        "few_shot": 0,
        "month": null,
        "year": "2019",
        "keywords": [
            ""
        ],
        "link": "https://arxiv.org/abs/1907.10641"
    },
    "ratio_filling": 1.0,
    "error": null
}