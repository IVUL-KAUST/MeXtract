{
    "metadata": {
        "Name": "GPQA",
        "Link": "https://github.com/idavidrein/gpqa/",
        "HF Link": "",
        "License": "unknown",
        "Year": 2022,
        "Language": "en",
        "Domain": [
            "social media",
            "news articles",
            "reviews",
            "commentary",
            "books",
            "wikipedia",
            "web pages"
        ],
        "Form": "text",
        "Collection Style": [
            "human annotation",
            "manual curation"
        ],
        "Description": "A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. The questions are designed to be difficult for both experts and state-of-the-art AI systems.",
        "Volume": 448.0,
        "Unit": "sentences",
        "Ethical Risks": "Medium",
        "Provider": [
            "New York University",
            "Cohere",
            "Anthropic"
        ],
        "Derived From": [],
        "Paper Title": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark",
        "Paper Link": "https://arxiv.org/abs/2311.12022",
        "Tokenized": false,
        "Host": "GitHub",
        "Access": "Free",
        "Cost": "",
        "Test Split": true,
        "Tasks": [
            "multiple choice question answering"
        ],
        "Venue Title": "NeurIPS",
        "Venue Type": "conference",
        "Venue Name": "",
        "Authors": [
            "David Rein",
            "Betty Li Hou",
            "Asa Cooper Stickland",
            "Jackson Petty",
            "Richard Yuanzhe Pang",
            "Julien Dirani",
            "Julian Michael",
            "Samuel R. Bowman"
        ],
        "Affiliations": [
            "New York University",
            "Cohere",
            "Anthropic, PBC"
        ],
        "Abstract": "We present DatasetName{}, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are ``Google-proof''). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4--based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions---for example, when developing new scientific knowledge---we need to develop textit{scalable oversight} methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of DatasetName{} both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities."
    },
    "validation": {
        "ACCESSABILITY": 0.7142857142857143,
        "DIVERSITY": 1.0,
        "CONTENT": 0.7142857142857143,
        "EVALUATION": 0.6666666666666666,
        "AVERAGE": 0.7222222222222222
    },
    "length_forcing": 0.9999999999999996,
    "cost": {
        "cost": 0.0029321,
        "input_tokens": 27352,
        "output_tokens": 665
    },
    "config": {
        "model_name": "google_gemma-3-27b-it",
        "few_shot": 0,
        "month": null,
        "year": "2023",
        "keywords": [
            ""
        ],
        "link": "https://arxiv.org/abs/2311.12022"
    },
    "ratio_filling": 1.0,
    "error": null
}