{
    "metadata": {
        "Name": "RuBia",
        "Link": "https://github.com/vergrig/RuBia-Dataset",
        "HF Link": "",
        "License": "CC BY 4.0",
        "Year": 2024,
        "Language": "ru",
        "Domain": [
            "social media"
        ],
        "Form": "text",
        "Collection Style": [
            "manual curation",
            "human annotation"
        ],
        "Description": "RuBia is a Russian language bias detection dataset divided into 4 domains: gender, nationality, socio-economic status, and diverse. Each example consists of two sentences, one reinforcing a harmful stereotype and the other contradicting it. The data was written by volunteers and validated by crowdworkers.",
        "Volume": 1989.0,
        "Unit": "sentences",
        "Ethical Risks": "High",
        "Provider": [
            "Queen\u2019s University",
            "Higher School of Economics",
            "Wildberries",
            "Linguistic Convergence Laboratory",
            "Toloka AI"
        ],
        "Derived From": [],
        "Paper Title": "RuBia: A Russian Language Bias Detection Dataset",
        "Paper Link": "https://arxiv.org/abs/2403.17553",
        "Tokenized": false,
        "Host": "GitHub",
        "Access": "Free",
        "Cost": "",
        "Test Split": false,
        "Tasks": [
            "other"
        ],
        "Venue Title": "arXiv",
        "Venue Type": "preprint",
        "Venue Name": "arXiv",
        "Authors": [
            "Veronika Grigoreva",
            "Anastasiia Ivanova",
            "Ilseyar Alimova",
            "Ekaterina Artemova"
        ],
        "Affiliations": [
            "Queen\u2019s University",
            "Higher School of Economics",
            "Wildberries",
            "Linguistic Convergence Laboratory",
            "Toloka AI"
        ],
        "Abstract": "Warning: this work contains upsetting or disturbing content. Large language models (LLMs) tend to learn the social and cultural biases present in the raw pre-training data. To test if a LLM\u2019s behavior is fair, functional datasets are employed, and due to their purpose, these datasets are highly language and culture-specific. In this paper, we address a gap in the scope of multilingual bias evaluation by presenting a bias detection dataset specifically designed for the Russian language, dubbed as RuBia. The RuBia dataset is divided into 4 domains: gender, nationality, socio-economic status, and diverse, each of the domains is further divided into multiple fine-grained subdomains. Every example in the dataset consists of two sentences with the first reinforcing a potentially harmful stereotype or trope and the second contradicting it. Thesesentencepairswerefirst written by volunteers and then validated by native-speaking crowdsourcing workers. Overall, there are nearly 2,000 unique sentence pairs spread over 19 subdomains in RuBia. To illustrate the dataset\u2019s purpose, we conduct a diagnostic evaluation of state-of-the-art or near-state-of-the-art LLMs and discuss the LLMs\u2019 predisposition to social biases."
    },
    "validation": {},
    "cost": {
        "cost": 0.06419125,
        "input_tokens": 18636,
        "output_tokens": 1597
    },
    "config": {
        "model_name": "google_gemini-2.5-pro",
        "few_shot": 0,
        "month": null,
        "year": null,
        "keywords": [
            ""
        ],
        "link": "https://arxiv.org/pdf/2403.17553"
    },
    "ratio_filling": 1.0,
    "error": null
}