{
    "metadata": {
        "Name": "RuBia",
        "Link": "http://github.com/vergrig/RuBia-Dataset",
        "HF Link": "",
        "License": "CC BY 4.0",
        "Year": 2023,
        "Language": "ru",
        "Domain": [
            "social media",
            "news articles",
            "books",
            "web pages"
        ],
        "Form": "text",
        "Collection Style": [
            "crawling",
            "human annotation"
        ],
        "Description": "RuBia is a Russian language bias detection dataset designed to evaluate social biases in large language models. It consists of 1989 unique sentence pairs across 4 domains: gender, nationality, socio-economic status, and diverse.",
        "Volume": 1989.0,
        "Unit": "sentences",
        "Ethical Risks": "Medium",
        "Provider": [
            "Queen's University",
            "Higher School of Economics",
            "Wildberries",
            "Linguistic Convergence Laboratory"
        ],
        "Derived From": [],
        "Paper Title": "RuBia: A Russian Language Bias Detection Dataset",
        "Paper Link": "http://github.com/vergrig/RuBia-Dataset",
        "Tokenized": false,
        "Host": "GitHub",
        "Access": "Free",
        "Cost": "",
        "Test Split": false,
        "Tasks": [
            "irony detection",
            "language modeling",
            "machine translation"
        ],
        "Venue Title": "RuBia: A Russian Language Bias Detection Dataset",
        "Venue Type": "preprint",
        "Venue Name": "",
        "Authors": [
            "Veronika Grigoreva",
            "Anastasiia Ivanova",
            "Ilseyar Alimova",
            "Ekaterina Artemova"
        ],
        "Affiliations": [
            "Queen's University",
            "Higher School of Economics",
            "Wildberries",
            "Linguistic Convergence Laboratory"
        ],
        "Abstract": "Large language models (LLM) tend to learn the social and cultural biases present in the raw pre-training data. To test if a LLM's behavior is fair, functional datasets are employed, and due to their purpose, these datasets are highly language and culture-specific. In this paper, we address a gap in the scope of multilingual bias evaluation by presenting a bias detection dataset specifically designed for the Russian language, dubbed as RuBia. The RuBia dataset is divided into 4 domains: gender, nationality, socio-economic status, and diverse, each of the domains is further divided into multiple fine-grained subdomains. Every example in the dataset consists of two sentences with the first reinforcing a potentially harmful stereotype or trope and the second contradicting it. These sentence pairs were first written by volunteers and then validated by native-speaking crowdsourcing workers. Overall, there are nearly 2,000 unique sentence pairs spread over 19 subdomains in RuBia. To illustrate the dataset's purpose, we conduct a diagnostic evaluation of state-of-the-art or near-state-of-the-art LLMs and discuss the LLMs' predisposition to social biases."
    },
    "validation": {
        "ACCESSABILITY": 0.5714285714285714,
        "DIVERSITY": 1.0,
        "CONTENT": 0.5714285714285714,
        "EVALUATION": 0.6666666666666666,
        "AVERAGE": 0.6111111111111112
    },
    "length_forcing": 0.9999999999999996,
    "cost": {
        "cost": 0.00222459,
        "input_tokens": 15891,
        "output_tokens": 637
    },
    "config": {
        "model_name": "qwen_qwen-2.5-72b-instruct",
        "few_shot": 0,
        "month": null,
        "year": "2024",
        "keywords": [
            ""
        ],
        "link": "https://arxiv.org/abs/2403.17553"
    },
    "ratio_filling": 1.0,
    "error": null
}