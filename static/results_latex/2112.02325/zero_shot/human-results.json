{
    "metadata": {
        "Name": "Russian Jeopardy",
        "Link": "https://github.com/evrog/Russian-QA-Jeopardy",
        "HF Link": "",
        "License": "custom",
        "Year": 2024,
        "Language": "ru",
        "Domain": [
            "web pages"
        ],
        "Form": "text",
        "Collection Style": [
            "crawling"
        ],
        "Description": "A dataset of 29,375 Russian quiz-show questions of the Jeopardy! type, collected from the ChGK database. The questions are fact-oriented and suitable for open-domain question answering tasks.",
        "Volume": 29375.0,
        "Unit": "sentences",
        "Ethical Risks": "Low",
        "Provider": [
            "Tyumen State University"
        ],
        "Derived From": [],
        "Paper Title": "Russian Jeopardy! Data Set for Question-Answering Systems",
        "Paper Link": "https://arxiv.org/pdf/2112.02325",
        "Tokenized": false,
        "Host": "GitHub",
        "Access": "Free",
        "Cost": "",
        "Test Split": false,
        "Tasks": [
            "question answering"
        ],
        "Venue Title": "ArXiv",
        "Venue Type": "preprint",
        "Venue Name": "",
        "Authors": [
            "Elena Mikhalkova",
            "Alexander Khlyupin"
        ],
        "Affiliations": [
            "Tyumen State University"
        ],
        "Abstract": "Question answering (QA) is one of the most common NLP tasks that relates to named entity recognition, fact extraction, semantic search and some other fields. In industry, it is much valued in chat-bots and corporate information systems. It is also a challenging task that attracted the attention of a very general audience at the quiz show Jeopardy! In this article we describe a Jeopardy!-like Russian QA dataset collected from the official Russian quiz database Chgk che-ge-\u2018ka:. The data set includes 379,284 quiz-like questions with 29,375 from the Russian analogue of Jeopardy! \u2013 'Own Game'. We observe its linguistic features and the related QA-task. We conclude about perspectives of a QA challenge based on the collected dataset.",
        "annotations_from_paper": {
            "Name": 1,
            "Link": 1,
            "HF Link": 0,
            "License": 1,
            "Year": 1,
            "Language": 1,
            "Domain": 1,
            "Form": 1,
            "Collection Style": 1,
            "Description": 1,
            "Volume": 1,
            "Unit": 1,
            "Ethical Risks": 1,
            "Provider": 1,
            "Derived From": 1,
            "Paper Title": 1,
            "Paper Link": 1,
            "Tokenized": 1,
            "Host": 1,
            "Access": 1,
            "Cost": 1,
            "Test Split": 1,
            "Tasks": 1,
            "Venue Title": 1,
            "Venue Type": 1,
            "Venue Name": 1,
            "Authors": 1,
            "Affiliations": 1,
            "Abstract": 1
        }
    },
    "validation": {
        "ACCESSABILITY": 1.0,
        "DIVERSITY": 1.0,
        "CONTENT": 1.0,
        "EVALUATION": 1.0,
        "AVERAGE": 1.0
    },
    "length_forcing": 0.9999999999999996,
    "cost": {
        "cost": 0,
        "input_tokens": 0,
        "output_tokens": 0
    },
    "config": {
        "model_name": "human",
        "few_shot": 0,
        "month": null,
        "year": "2024",
        "keywords": [
            ""
        ],
        "link": "https://arxiv.org/abs/2112.02325"
    },
    "ratio_filling": 1.0,
    "error": null
}