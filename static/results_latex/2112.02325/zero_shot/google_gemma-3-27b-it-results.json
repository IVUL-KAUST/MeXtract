{
    "metadata": {
        "Name": "Russian Jeopardy! Data Set",
        "Link": "https://github.com/evrog/Russian-QA-Jeopardy",
        "HF Link": "",
        "License": "unknown",
        "Year": 2022,
        "Language": "ru",
        "Domain": [
            "other"
        ],
        "Form": "text",
        "Collection Style": [
            "manual curation"
        ],
        "Description": "A dataset of 29,375 Russian Jeopardy!-like questions and answers collected from the official Russian quiz database Chgk. The questions are shorter and more fact-oriented than typical ChGK questions.",
        "Volume": 29375.0,
        "Unit": "tokens",
        "Ethical Risks": "Low",
        "Provider": [
            "Tyumen State University"
        ],
        "Derived From": [],
        "Paper Title": "Russian Jeopardy! Data Set for Question-Answering Systems",
        "Paper Link": "https://arxiv.org/abs/2112.02325",
        "Tokenized": false,
        "Host": "GitHub",
        "Access": "Free",
        "Cost": "",
        "Test Split": false,
        "Tasks": [
            "question answering"
        ],
        "Venue Title": "LREC 2022",
        "Venue Type": "conference",
        "Venue Name": "",
        "Authors": [
            "Elena Mikhalkova",
            "Alexander Khlyupin"
        ],
        "Affiliations": [
            "Tyumen State University"
        ],
        "Abstract": "Question answering (QA) is one of the most common NLP tasks that relates to named entity recognition, fact extraction, semantic search and some other fields. In industry, it is much valued in chat-bots and corporate information systems. It is also a challenging task that attracted the attention of a very general audience at the quiz show Jeopardy! In this article we describe a Jeopardy!-like Russian QA data set collected from the official Russian quiz database Chgk {em che-ge-`ka:}. The data set includes 379,284 quiz-like questions with 29,375 from the Russian analogue of Jeopardy! -- ``Own Game''. We observe its linguistic features and the related QA-task. We conclude about perspectives of a QA challenge based on the collected data set."
    },
    "validation": {
        "ACCESSABILITY": 0.8571428571428571,
        "DIVERSITY": 1.0,
        "CONTENT": 0.5714285714285714,
        "EVALUATION": 1.0,
        "AVERAGE": 0.7777777777777778
    },
    "length_forcing": 0.9999999999999996,
    "cost": {
        "cost": 0.001488686,
        "input_tokens": 11238,
        "output_tokens": 498
    },
    "config": {
        "model_name": "google_gemma-3-27b-it",
        "few_shot": 0,
        "month": null,
        "year": "2024",
        "keywords": [
            ""
        ],
        "link": "https://arxiv.org/abs/2112.02325"
    },
    "ratio_filling": 1.0,
    "error": null
}