{
    "metadata": {
        "Name": "Russian Jeopardy! Data Set",
        "Link": "https://github.com/evrog/Russian-QA-Jeopardy",
        "HF Link": "",
        "License": "CC BY-NC 4.0",
        "Year": 2021,
        "Language": "ru",
        "Domain": [
            "social media"
        ],
        "Form": "text",
        "Collection Style": [
            "manual curation"
        ],
        "Description": "A dataset of 29,375 Russian Jeopardy! questions and answers collected from the official Russian quiz database Chgk.",
        "Volume": 29375.0,
        "Unit": "tokens",
        "Ethical Risks": "Low",
        "Provider": [
            "Tyumen State University"
        ],
        "Derived From": [
            "Chgk database"
        ],
        "Paper Title": "Russian Jeopardy! Data Set for Question-Answering Systems",
        "Paper Link": "https://arxiv.org/abs/2112.02325",
        "Tokenized": false,
        "Host": "GitHub",
        "Access": "Free",
        "Cost": "",
        "Test Split": true,
        "Tasks": [
            "question answering"
        ],
        "Venue Title": "LREC",
        "Venue Type": "conference",
        "Venue Name": "Language Resources and Evaluation Conference",
        "Authors": [
            "Elena Mikhalkova",
            "Alexander Khlyupin"
        ],
        "Affiliations": [
            "Tyumen State University"
        ],
        "Abstract": "Question answering (QA) is one of the most common NLP tasks that relates to named entity recognition, fact extraction, semantic search and some other fields. In industry, it is much valued in chat-bots and corporate information systems. It is also a challenging task that attracted the attention of a very general audience at the quiz show Jeopardy! In this article we describe a Jeopardy!-like Russian QA data set collected from the official Russian quiz database Chgk. The data set includes 379,284 quiz-like questions with 29,375 from the Russian analogue of Jeopardy! -- 'Own Game'. We observe its linguistic features and the related QA-task. We conclude about perspectives of a QA challenge based on the collected data set."
    },
    "validation": {
        "ACCESSABILITY": 0.8571428571428571,
        "DIVERSITY": 1.0,
        "CONTENT": 0.5714285714285714,
        "EVALUATION": 0.3333333333333333,
        "AVERAGE": 0.6666666666666666
    },
    "length_forcing": 0.9999999999999996,
    "cost": {
        "cost": 0.00350276,
        "input_tokens": 11226,
        "output_tokens": 460
    },
    "config": {
        "model_name": "deepseek_deepseek-chat-v3-0324",
        "few_shot": 0,
        "month": null,
        "year": "2024",
        "keywords": [
            ""
        ],
        "link": "https://arxiv.org/abs/2112.02325"
    },
    "ratio_filling": 1.0,
    "error": null
}