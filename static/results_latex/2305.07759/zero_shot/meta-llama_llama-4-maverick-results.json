{
    "metadata": {
        "Name": "TinyStories",
        "Link": "https://huggingface.co/datasets/roneneldan/TinyStories",
        "HF Link": "https://huggingface.co/datasets/roneneldan/TinyStories",
        "License": "MIT License",
        "Year": 2023,
        "Language": "en",
        "Domain": [
            "books"
        ],
        "Form": "text",
        "Collection Style": [
            "LLM generated"
        ],
        "Description": "A dataset of short stories for training small language models",
        "Volume": 2200.0,
        "Unit": "documents",
        "Ethical Risks": "Low",
        "Provider": [
            "Microsoft Research"
        ],
        "Derived From": [],
        "Paper Title": "TinyStories: How Small Can Language Models Be and Still Speak Coherent English?",
        "Paper Link": "https://arxiv.org/abs/2305.07759",
        "Tokenized": false,
        "Host": "HuggingFace",
        "Access": "Free",
        "Cost": "",
        "Test Split": true,
        "Tasks": [
            "language modeling",
            "text generation"
        ],
        "Venue Title": "ArXiv",
        "Venue Type": "preprint",
        "Venue Name": "",
        "Authors": [
            "Ronen Eldan",
            "Yuanzhi Li"
        ],
        "Affiliations": [
            "Microsoft Research"
        ],
        "Abstract": "Language models are powerful tools for natural language processing, but they often struggle to produce coherent and fluent text when they are small. This paper introduces TinyStories, a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We show that TinyStories can be used to train and evaluate LMs that are much smaller than the state-of-the-art models, yet still produce fluent and consistent stories with several paragraphs that are diverse and have almost perfect grammar, and demonstrate reasoning capabilities."
    },
    "validation": {
        "ACCESSABILITY": 0.8571428571428571,
        "DIVERSITY": 1.0,
        "CONTENT": 0.7142857142857143,
        "EVALUATION": 0.6666666666666666,
        "AVERAGE": 0.7777777777777778
    },
    "length_forcing": 0.9999999999999996,
    "cost": {
        "cost": 0.005367,
        "input_tokens": 34870,
        "output_tokens": 418
    },
    "config": {
        "model_name": "meta-llama_llama-4-maverick",
        "few_shot": 0,
        "month": null,
        "year": "2023",
        "keywords": [
            ""
        ],
        "link": "https://arxiv.org/abs/2305.07759"
    },
    "ratio_filling": 1.0,
    "error": null
}