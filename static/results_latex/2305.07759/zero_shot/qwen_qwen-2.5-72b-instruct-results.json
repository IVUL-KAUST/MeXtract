{
    "metadata": {
        "Name": "TinyStories",
        "Link": "https://huggingface.co/datasets/tiny_stories",
        "HF Link": "https://huggingface.co/datasets/tiny_stories",
        "License": "unknown",
        "Year": 2023,
        "Language": "en",
        "Domain": [
            "reviews"
        ],
        "Form": "text",
        "Collection Style": [
            "LLM generated"
        ],
        "Description": "A synthetic dataset of short stories generated by GPT-3.5 and GPT-4, containing only words that a typical 3 to 4-year-old child would understand. The dataset is designed to train and evaluate small language models.",
        "Volume": 1000000.0,
        "Unit": "sentences",
        "Ethical Risks": "Low",
        "Provider": [
            "Microsoft Research"
        ],
        "Derived From": [
            "GPT-3.5",
            "GPT-4"
        ],
        "Paper Title": "TinyStories: How Small Can Language Models Be and Still Speak Coherent English?",
        "Paper Link": "https://arxiv.org/abs/2304.00000",
        "Tokenized": false,
        "Host": "HuggingFace",
        "Access": "Free",
        "Cost": "",
        "Test Split": true,
        "Tasks": [
            "language modeling",
            "irony detection",
            "instruction tuning"
        ],
        "Venue Title": "TinyStories: How Small Can Language Models Be and Still Speak Coherent English?",
        "Venue Type": "preprint",
        "Venue Name": "",
        "Authors": [
            "Ronen Eldan",
            "Yuanzhi Li"
        ],
        "Affiliations": [
            "Microsoft Research"
        ],
        "Abstract": "Language models (LMs) are powerful tools for natural language processing, but they often struggle to produce coherent and fluent text when they are small. Models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can rarely generate coherent and consistent English text beyond a few words even after extensive training. This raises the question of whether the emergence of the ability to produce coherent English text only occurs at larger scales and complex architectures. In this work, we introduce TinyStories, a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We show that TinyStories can be used to train and evaluate LMs that are much smaller than the state-of-the-art models (below 10 million total parameters), or have much simpler architectures (with only one transformer block), yet still produce fluent and consistent stories with several paragraphs that are diverse and have almost perfect grammar, and demonstrate reasoning capabilities."
    },
    "validation": {
        "ACCESSABILITY": 0.5714285714285714,
        "DIVERSITY": 1.0,
        "CONTENT": 0.5714285714285714,
        "EVALUATION": 0.0,
        "AVERAGE": 0.5
    },
    "length_forcing": 0.9999999999999996,
    "cost": {
        "cost": 0.0048471,
        "input_tokens": 34867,
        "output_tokens": 568
    },
    "config": {
        "model_name": "qwen_qwen-2.5-72b-instruct",
        "few_shot": 0,
        "month": null,
        "year": "2023",
        "keywords": [
            ""
        ],
        "link": "https://arxiv.org/abs/2305.07759"
    },
    "ratio_filling": 1.0,
    "error": null
}