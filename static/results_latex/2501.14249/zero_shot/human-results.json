{
    "metadata": {
        "Name": "HLE",
        "Link": "https://lastexam.ai",
        "HF Link": "https://huggingface.co/datasets/cais/hle",
        "License": "MIT License",
        "Year": 2025,
        "Language": "en",
        "Domain": [
            "other"
        ],
        "Form": "text",
        "Collection Style": [
            "human annotation",
            "manual curation"
        ],
        "Description": "Humanity's Last Exam (HLE) is a dataset of 3,000 challenging questions designed to assess the capabilities of large language models (LLMs). The questions are diverse, covering a wide range of topics and requiring different reasoning abilities. The dataset is still under development and accepting new questions.",
        "Volume": 3000.0,
        "Unit": "sentences",
        "Ethical Risks": "Low",
        "Provider": [
            "Center for AI Safety",
            "Scale AI"
        ],
        "Derived From": [],
        "Paper Title": "Humanity's Last Exam",
        "Paper Link": "https://arxiv.org/pdf/2501.14249",
        "Tokenized": false,
        "Host": "other",
        "Access": "Free",
        "Cost": "",
        "Test Split": false,
        "Tasks": [
            "question answering",
            "multiple choice question answering"
        ],
        "Venue Title": "arXiv",
        "Venue Type": "preprint",
        "Venue Name": "",
        "Authors": [
            "Long Phan",
            "Alice Gatti",
            "Ziwen Han",
            "Nathaniel Li",
            "Josephina Hu",
            "Hugh Zhang",
            "Sean Shi",
            "Michael Choi",
            "Anish Agrawal",
            "Arnav Chopra",
            "Adam Khoja",
            "Ryan Kim",
            "Richard Ren",
            "Jason Hausenloy",
            "Oliver Zhang",
            "Mantas Mazeika",
            "Summer Yue",
            "Alexandr Wang",
            "Dan Hendrycks"
        ],
        "Affiliations": [
            "Center for AI Safety",
            "Scale AI"
        ],
        "Abstract": "We introduce Humanity's Last Exam (HLE), a dataset of 3,000 challenging questions designed to assess the capabilities of large language models (LLMs). The questions are diverse, covering a wide range of topics and requiring different reasoning abilities. We evaluate a variety of LLMs on HLE, finding that even the most advanced models struggle with many of the questions. We believe that HLE is a valuable resource for researchers working to develop more capable and robust LLMs. We are still accepting new questions for HLE, and we encourage researchers to submit their own challenging questions to help us build a more comprehensive and challenging benchmark.",
        "annotations_from_paper": {
            "Name": 1,
            "Link": 1,
            "HF Link": 0,
            "License": 0,
            "Year": 1,
            "Language": 1,
            "Domain": 1,
            "Form": 1,
            "Collection Style": 1,
            "Description": 1,
            "Volume": 1,
            "Unit": 1,
            "Ethical Risks": 1,
            "Provider": 1,
            "Derived From": 1,
            "Paper Title": 1,
            "Paper Link": 1,
            "Tokenized": 1,
            "Host": 1,
            "Access": 1,
            "Cost": 1,
            "Test Split": 1,
            "Tasks": 1,
            "Venue Title": 1,
            "Venue Type": 1,
            "Venue Name": 1,
            "Authors": 1,
            "Affiliations": 1,
            "Abstract": 1
        }
    },
    "validation": {
        "ACCESSABILITY": 1.0,
        "DIVERSITY": 1.0,
        "CONTENT": 1.0,
        "EVALUATION": 1.0,
        "AVERAGE": 1.0
    },
    "length_forcing": 0.9999999999999996,
    "cost": {
        "cost": 0,
        "input_tokens": 0,
        "output_tokens": 0
    },
    "config": {
        "model_name": "human",
        "few_shot": 0,
        "month": null,
        "year": "2025",
        "keywords": [
            ""
        ],
        "link": "https://arxiv.org/abs/2501.14249"
    },
    "ratio_filling": 1.0,
    "error": null
}