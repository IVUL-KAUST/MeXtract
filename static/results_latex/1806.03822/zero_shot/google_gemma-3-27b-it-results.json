{
    "metadata": {
        "Name": "SQuAD 2.0",
        "Link": "https://bit.ly/2rDHBgY",
        "HF Link": "",
        "License": "CC BY-SA 4.0",
        "Year": 2018,
        "Language": "en",
        "Domain": [
            "wikipedia"
        ],
        "Form": "text",
        "Collection Style": [
            "human annotation"
        ],
        "Description": "Combines existing SQuAD data with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones.",
        "Volume": 130319.0,
        "Unit": "images",
        "Ethical Risks": "Low",
        "Provider": [
            "Stanford University"
        ],
        "Derived From": [
            "SQuAD 1.1"
        ],
        "Paper Title": "Know What You Don't Know: Unanswerable Questions for SQuAD",
        "Paper Link": "https://arxiv.org/abs/1806.03822",
        "Tokenized": false,
        "Host": "CodaLab",
        "Access": "Free",
        "Cost": "",
        "Test Split": true,
        "Tasks": [
            "question answering"
        ],
        "Venue Title": "ACL",
        "Venue Type": "conference",
        "Venue Name": "",
        "Authors": [
            "Pranav Rajpurkar",
            "Robin Jia",
            "Percy Liang"
        ],
        "Affiliations": [
            "Computer Science Department, Stanford University"
        ],
        "Abstract": "Machine reading comprehension has become a central task in natural language understanding, fueled by the creation of many large-scale datasets. Nonetheless, these systems are still far from true language understanding. Recent analysis shows that models can do well at SQuAD by learning context and type-matching heuristics, and that success on SQuAD does not ensure robustness to distracting sentences. One root cause of these problems is SQuAD's focus on questions for which a correct answer is guaranteed to exist in the context document. Therefore, models only need to select the span that seems most related to the question, instead of checking that the answer is actually entailed by the text. In this work, we construct SQuAD 2.0, a new dataset that combines answerable questions from the previous version of SQuAD with 53,775 new, unanswerable questions about the same paragraphs. Crowdworkers crafted these questions so that (1) they are relevant to the paragraph, and (2) the paragraph contains a plausible answer---something of the same type as what the question asks for. We confirm that SQuAD 2.0 is both challenging and high-quality. A state-of-the-art model achieves only 66.3% F1 score when trained and tested on SQuAD 2.0, whereas human accuracy is 89.5% F1, a full 23.2 points higher. The same model architecture trained on SQuAD 1.1 gets 85.8% F1, only 5.4 points worse than humans. We also show that our unanswerable questions are more challenging than ones created automatically, either via distant supervision or a rule-based method. We release SQuAD 2.0 to the public as new version of SQuAD, and make it the primary benchmark on the official SQuAD leaderboard. We are optimistic that this new dataset will encourage the development of reading comprehension systems that know what they don't know."
    },
    "validation": {
        "ACCESSABILITY": 0.5714285714285714,
        "DIVERSITY": 1.0,
        "CONTENT": 0.7142857142857143,
        "EVALUATION": 1.0,
        "AVERAGE": 0.7222222222222222
    },
    "length_forcing": 0.9999999999999996,
    "cost": {
        "cost": 0.0016921,
        "input_tokens": 14628,
        "output_tokens": 746
    },
    "config": {
        "model_name": "google_gemma-3-27b-it",
        "few_shot": 0,
        "month": null,
        "year": "2018",
        "keywords": [
            ""
        ],
        "link": "https://arxiv.org/abs/1806.03822"
    },
    "ratio_filling": 1.0,
    "error": null
}