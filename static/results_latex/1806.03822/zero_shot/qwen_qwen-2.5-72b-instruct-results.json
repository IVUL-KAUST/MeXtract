{
    "metadata": {
        "Name": "SQuAD 2.0",
        "Link": "https://bit.ly/2rDHBgY",
        "HF Link": "",
        "License": "CC BY-SA 4.0",
        "Year": 2018,
        "Language": "en",
        "Domain": [
            "news articles"
        ],
        "Form": "text",
        "Collection Style": [
            "human annotation"
        ],
        "Description": "SQuAD 2.0 is an extension of the Stanford Question Answering Dataset (SQuAD) that includes over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones.",
        "Volume": 130319.0,
        "Unit": "tokens",
        "Ethical Risks": "Low",
        "Provider": [
            "Stanford University"
        ],
        "Derived From": [
            "SQuAD 1.1"
        ],
        "Paper Title": "Know What You Don't Know: Unanswerable Questions for SQuAD",
        "Paper Link": "https://bit.ly/2rDHBgY",
        "Tokenized": false,
        "Host": "CodaLab",
        "Access": "Free",
        "Cost": "",
        "Test Split": true,
        "Tasks": [
            "machine translation",
            "question answering"
        ],
        "Venue Title": "ACL",
        "Venue Type": "conference",
        "Venue Name": "",
        "Authors": [
            "Pranav Rajpurkar",
            "Robin Jia",
            "Percy Liang"
        ],
        "Affiliations": [
            "Computer Science Department, Stanford University"
        ],
        "Abstract": "Machine reading comprehension has become a central task in natural language understanding, fueled by the creation of many large-scale datasets. In this work, we construct SQuAD 2.0, a new dataset that combines answerable questions from the previous version of SQuAD (SQuAD 1.1) with 53,775 new, unanswerable questions about the same paragraphs. Crowdworkers crafted these questions so that they are relevant to the paragraph and the paragraph contains a plausible answer. We confirm that SQuAD 2.0 is both challenging and high-quality. A state-of-the-art model achieves only 66.3% F1 score when trained and tested on SQuAD 2.0, whereas human accuracy is 89.5% F1, a full 23.2 points higher."
    },
    "validation": {
        "ACCESSABILITY": 0.5714285714285714,
        "DIVERSITY": 1.0,
        "CONTENT": 0.5714285714285714,
        "EVALUATION": 1.0,
        "AVERAGE": 0.6666666666666666
    },
    "length_forcing": 0.9999999999999996,
    "cost": {
        "cost": 0.00214132,
        "input_tokens": 14625,
        "output_tokens": 507
    },
    "config": {
        "model_name": "qwen_qwen-2.5-72b-instruct",
        "few_shot": 0,
        "month": null,
        "year": "2018",
        "keywords": [
            ""
        ],
        "link": "https://arxiv.org/abs/1806.03822"
    },
    "ratio_filling": 1.0,
    "error": null
}