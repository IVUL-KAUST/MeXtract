{
    "metadata": {
        "Name": "TriviaQA",
        "Link": "http://nlp.cs.washington.edu/triviaqa/",
        "HF Link": "",
        "License": "unknown",
        "Year": 2017,
        "Language": "en",
        "Domain": [
            "wikipedia",
            "web pages"
        ],
        "Form": "text",
        "Collection Style": [
            "crawling"
        ],
        "Description": "TriviaQA is a large-scale reading comprehension dataset with over 650,000 question-answer-evidence triples. Questions are from trivia enthusiasts, and evidence is from Wikipedia and web search.",
        "Volume": 662659.0,
        "Unit": "documents",
        "Ethical Risks": "Low",
        "Provider": [
            "University of Washington",
            "Allen Institute for Artificial Intelligence"
        ],
        "Derived From": [],
        "Paper Title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension",
        "Paper Link": "https://arxiv.org/abs/1705.03551",
        "Tokenized": false,
        "Host": "other",
        "Access": "Free",
        "Cost": "",
        "Test Split": true,
        "Tasks": [
            "question answering",
            "information retrieval"
        ],
        "Venue Title": "ACL 2017",
        "Venue Type": "conference",
        "Venue Name": "Association for Computational Linguistics",
        "Authors": [
            "Mandar Joshi",
            "Eunsol Choi",
            "Daniel S. Weld",
            "Luke Zettlemoyer"
        ],
        "Affiliations": [
            "Paul G. Allen School of Computer Science & Engineering, Univ. of Washington, Seattle, WA",
            "Allen Institute for Artificial Intelligence, Seattle, WA"
        ],
        "Abstract": "We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23% and 40% vs. 80%), suggesting that TriviaQA is a challenging testbed that is worth significant future study. Data and code available at http://nlp.cs.washington.edu/triviaqa/"
    },
    "validation": {
        "ACCESSABILITY": 0.5714285714285714,
        "DIVERSITY": 1.0,
        "CONTENT": 0.8571428571428571,
        "EVALUATION": 1.0,
        "AVERAGE": 0.7777777777777778
    },
    "length_forcing": 0.9999999999999996,
    "cost": {
        "cost": 0.0990475,
        "input_tokens": 14648,
        "output_tokens": 588
    },
    "config": {
        "model_name": "google_gemini-2.5-pro-preview-03-25",
        "few_shot": 1,
        "month": null,
        "year": "2017",
        "keywords": [
            ""
        ],
        "link": "https://arxiv.org/abs/1705.03551"
    },
    "ratio_filling": 1.0,
    "error": null
}