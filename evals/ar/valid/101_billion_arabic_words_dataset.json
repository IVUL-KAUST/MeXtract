{
    "Name": "101 Billion Arabic Words Dataset",
    "Subsets": [],
    "HF_Link": "https://hf.co/datasets/ClusterlabAi/101_billion_arabic_words_dataset",
    "Link": "https://hf.co/datasets/ClusterlabAi/101_billion_arabic_words_dataset",
    "License": "Apache-2.0",
    "Year": 2024,
    "Language": "ar",
    "Dialect": "mixed",
    "Domain": [
        "web pages"
    ],
    "Form": "text",
    "Collection_Style": [
        "crawling"
    ],
    "Description": "The 101 Billion Arabic Words Dataset is curated by the Clusterlab team and consists of 101 billion words extracted and cleaned from web content, specifically targeting Arabic text. This dataset is intended for use in natural language processing applications, particularly in training and fine-tuning Large Language Models (LLMs)",
    "Volume": 101000000000,
    "Unit": "tokens",
    "Ethical_Risks": "High",
    "Provider": [
        "Clusterlab"
    ],
    "Derived_From": [
        "Common Crawl"
    ],
    "Paper_Title": "101 Billion Arabic Words Dataset",
    "Paper_Link": "https://arxiv.org/pdf/2405.01590v1",
    "Script": "Arab",
    "Tokenized": false,
    "Host": "HuggingFace",
    "Access": "Free",
    "Cost": "",
    "Test_Split": false,
    "Tasks": [
        "text generation",
        "language modeling"
    ],
    "Venue_Title": "arXiv",
    
    "Venue_Type": "preprint",
    "Venue_Name": "",
    "Authors": [
        "Manel Aloui",
        "Hasna Chouikhi",
        "Ghaith Chaabane",
        "Haithem Kchaou",
        "Chehir Dhaouadi"
    ],
    "Affiliations": [
        "Clusterlab"
    ],
    "Abstract": "In recent years, Large Language Models (LLMs) have revolutionized the field of natural language processing, showcasing an impressive rise predominantly in English-centric domains. These advancements have set a global benchmark, inspiring significant efforts toward developing Arabic LLMs capable of understanding and generating the Arabic language with remarkable accuracy. Despite these advancements, a critical challenge persists: the potential bias in Arabic LLMs, primarily attributed to their reliance on datasets comprising English data that has been translated into Arabic. This reliance not only compromises the authenticity of the generated content but also reflects a broader issue\u2014the scarcity of original quality Arabic linguistic data. This study aims to address the data scarcity in the Arab world and to encourage the development of Arabic Language Models that are true to both the linguistic and nuances of the region. We undertook a large-scale data mining project, extracting a substantial volume of text from the Common Crawl WET files, specifically targeting Arabic content. The extracted data underwent a rigorous cleaning and deduplication process, using innovative techniques to ensure the integrity and uniqueness of the dataset. The result is the 101 Billion Arabic Words Dataset, the largest Arabic dataset available to date, which can significantly contribute to the development of authentic Arabic LLMs. This study not only highlights the potential for creating linguistically and culturally accurate Arabic LLMs but also sets a precedent for future research in enhancing the authenticity of Arabic language models.",
    
    "annotations_from_paper": {
        "Name": 1,
        "Subsets": 1,
        "HF_Link": 1,
        "Link": 1,
        "License": 1,
        "Year": 1,
        "Language": 1,
        "Dialect": 1,
        "Domain": 1,
        "Form": 1,
        "Collection_Style": 1,
        "Description": 1,
        "Volume": 1,
        "Unit": 1,
        "Ethical_Risks": 1,
        "Provider": 1,
        "Derived_From": 1,
        "Paper_Title": 1,
        "Paper_Link": 1,
        "Script": 1,
        "Tokenized": 1,
        "Host": 1,
        "Access": 1,
        "Cost": 1,
        "Test_Split": 1,
        "Tasks": 0,
        "Venue_Title": 1,
        "Venue_Type": 1,
        "Venue_Name": 1,
        "Authors": 1,
        "Affiliations": 1,
        "Abstract": 1
        }
}