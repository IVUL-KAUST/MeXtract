{
    "Name": "POLYGLOT-NER",
    "Subsets": [],
    "HF_Link": "https://huggingface.co/datasets/rmyeid/polyglot_ner",
    "Link": "https://huggingface.co/datasets/rmyeid/polyglot_ner",
    "License": "unknown",
    "Year": 2014,
    "Language": "multilingual",
    "Dialect": "Modern Standard Arabic",
    "Domain": [
        "wikipedia"
    ],
    "Form": "text",
    "Collection_Style": [
        "machine annotation"
    ],
    "Description": "Polyglot-NER A training dataset automatically generated from Wikipedia and Freebase the task of named entity recognition. The dataset contains the basic Wikipedia based training data for 40 languages we have (with coreference resolution) for the task of named entity recognition.",
    "Volume": 10000144.0,
    "Unit": "tokens",
    "Ethical_Risks": "Low",
    "Provider": [
        "Stony Brook University"
    ],
    "Derived_From": [],
    "Paper_Title": "POLYGLOT-NER: Massive Multilingual Named Entity Recognition",
    "Paper_Link": "https://arxiv.org/pdf/1410.3791",
    "Script": "Arab",
    "Tokenized": false,
    "Host": "HuggingFace",
    "Access": "Free",
    "Cost": "",
    "Test_Split": false,
    "Tasks": [
        "named entity recognition"
    ],
    "Venue_Title": "arXiv",
    "Venue_Type": "preprint",
    "Venue_Name": "",
    "Authors": [
        "Rami Al-Rfou",
        "Vivek Kulkarni",
        "Bryan Perozzi",
        "Steven Skiena"
    ],
    "Affiliations": [
        "Stony Brook University"
    ],
    "Abstract": "The increasing diversity of languages used on the web introduces a new level of complexity to Information Retrieval (IR) systems. We can no longer assume that textual content is written in one language or even the same language family. In this paper, we demonstrate how to build massive multilingual annotators with minimal human expertise and intervention. We describe a system that builds Named Entity Recognition (NER) annotators for 40 major languages using Wikipedia and Freebase. Our approach does not require NER human annotated datasets or language specific resources like treebanks, parallel corpora, and orthographic rules. The novelty of approach lies therein - using only language agnostic techniques, while achieving competitive performance.   Our method learns distributed word representations (word embeddings) which encode semantic and syntactic features of words in each language. Then, we automatically generate datasets from Wikipedia link structure and Freebase attributes. Finally, we apply two preprocessing stages (oversampling and exact surface form matching) which do not require any linguistic expertise.   Our evaluation is two fold: First, we demonstrate the system performance on human annotated datasets. Second, for languages where no gold-standard benchmarks are available, we propose a new method, distant evaluation, based on statistical machine translation.",
    "annotations_from_paper": {
        "Name": 1,
        "Subsets": 1,
        "HF_Link": 0,
        "Link": 0,
        "License": 0,
        "Year": 1,
        "Language": 1,
        "Dialect": 1,
        "Domain": 1,
        "Form": 1,
        "Collection_Style": 1,
        "Description": 1,
        "Volume": 0,
        "Unit": 1,
        "Ethical_Risks": 1,
        "Provider": 1,
        "Derived_From": 1,
        "Paper_Title": 1,
        "Paper_Link": 1,
        "Script": 1,
        "Tokenized": 1,
        "Host": 0,
        "Access": 0,
        "Cost": 0,
        "Test_Split": 1,
        "Tasks": 1,
        "Venue_Title": 1,
        "Venue_Type": 1,
        "Venue_Name": 1,
        "Authors": 1,
        "Affiliations": 1,
        "Abstract": 1
    }
}