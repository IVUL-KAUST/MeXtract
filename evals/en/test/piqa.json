{
    "Name": "PIQA",
    "Link": "http://yonatanbisk.com/piqa",
    "HF_Link": "https://huggingface.co/datasets/ybisk/piqa",
    "License": "AFL-3.0",
    "Year": 2019,
    "Language": "en",
    "Domain": [
        "web pages"
    ],
    "Form": "text",
    "Collection_Style": [
        "human annotation"
    ],
    "Description": "A benchmark dataset for physical commonsense reasoning, presented as multiple-choice question answering. It contains goal-solution pairs inspired by how-to instructions from instructables.com, designed to test a model's understanding of physical properties, affordances, and object manipulation. The dataset was cleaned of artifacts using the AFLite algorithm.",
    "Volume": 21000.0,
    "Unit": "sentences",
    "Ethical_Risks": "Low",
    "Provider": [
        "Allen Institute for Artificial Intelligence",
        "Microsoft Research AI",
        "Carnegie Mellon University",
        "University of Washington"
    ],
    "Derived_From": [],
    "Paper_Title": "PIQA: Reasoning about Physical Commonsense in Natural Language",
    "Paper_Link": "https://arxiv.org/pdf/1911.11641",
    "Tokenized": false,
    "Host": "GitHub",
    "Access": "Free",
    "Cost": "",
    "Test_Split": true,
    "Tasks": [
        "question answering",
        "multiple choice question answering",
        "commonsense reasoning"
    ],
    "Venue_Title": "AAAI",
    "Venue_Type": "conference",
    "Venue_Name": "AAAI Conference on Artificial Intelligence",
    "Authors": [
        "Yonatan Bisk",
        "Rowan Zellers",
        "Ronan Le Bras",
        "Jianfeng Gao",
        "Yejin Choi"
    ],
    "Affiliations": [
        "Allen Institute for Artificial Intelligence",
        "Microsoft Research AI",
        "Carnegie Mellon University",
        "Paul G. Allen School for Computer Science and Engineering, University of Washington"
    ],
    "Abstract": "To apply eyeshadow without a brush, should I use a cotton swab or a toothpick? Questions requiring this kind of physical commonsense pose a challenge to today's natural language understanding systems. While recent pretrained models (such as BERT) have made progress on question answering over more abstract domains - such as news articles and encyclopedia entries, where text is plentiful - in more physical domains, text is inherently limited due to reporting bias. Can AI systems learn to reliably answer physical common-sense questions without experiencing the physical world? In this paper, we introduce the task of physical commonsense reasoning and a corresponding benchmark dataset Physical Interaction: Question Answering or PIQA. Though humans find the dataset easy (95% accuracy), large pretrained models struggle (77%). We provide analysis about the dimensions of knowledge that existing models lack, which offers significant opportunities for future research.",
    "annotations_from_paper": {
        "Name": 1,
        "Link": 1,
        "HF_Link": 0,
        "License": 0,
        "Year": 1,
        "Language": 1,
        "Domain": 1,
        "Form": 1,
        "Collection_Style": 1,
        "Description": 1,
        "Volume": 1,
        "Unit": 1,
        "Ethical_Risks": 1,
        "Provider": 1,
        "Derived_From": 1,
        "Paper_Title": 1,
        "Paper_Link": 1,
        "Tokenized": 1,
        "Host": 0,
        "Access": 1,
        "Cost": 1,
        "Test_Split": 1,
        "Tasks": 1,
        "Venue_Title": 1,
        "Venue_Type": 1,
        "Venue_Name": 1,
        "Authors": 1,
        "Affiliations": 1,
        "Abstract": 1
    }
}