{
    "Name": "HellaSwag",
    "Link": "https://rowanzellers.com/hellaswag",
    "HF_Link": "https://huggingface.co/datasets/Rowan/hellaswag",
    "License": "MIT License",
    "Year": 2019,
    "Language": "en",
    "Domain": [
        "captions",
        "public datasets",
        "wikipedia"
    ],
    "Form": "text",
    "Collection_Style": [
        "crawling",
        "machine annotation"
    ],
    "Description": "HellaSwag is a dataset for physically situated commonsense reasoning.",
    "Volume": 70000.0,
    "Unit": "sentences",
    "Ethical_Risks": "Low",
    "Provider": [
        "Allen Institute of Artificial Intelligence"
    ],
    "Derived_From": [
        "ActivityNet"
    ],
    "Paper_Title": "HellaSwag: Can a Machine Really Finish Your Sentence?",
    "Paper_Link": "https://arxiv.org/pdf/1905.07830",
    "Tokenized": false,
    "Host": "other",
    "Access": "Free",
    "Cost": "",
    "Test_Split": true,
    "Tasks": [
        "natural language inference"
    ],
    "Venue_Title": "arXiv",
    "Venue_Type": "preprint",
    "Venue_Name": "",
    "Authors": [
        "Rowan Zellers",
        "Ari Holtzman",
        "Yonatan Bisk",
        "Ali Farhadi",
        "Yejin Choi"
    ],
    "Affiliations": [
        "University of Washington",
        "Allen Institute of Artificial Intelligence"
    ],
    "Abstract": "Recent work by Zellers et al. (2018) introduced a new task of commonsense natural language inference: given an event description such as \"A woman sits at a piano,\" a machine must select the most likely followup: \"She sets her fingers on the keys.\" With the introduction of BERT, near human-level performance was reached. Does this mean that machines can perform human level commonsense inference?   In this paper, we show that commonsense inference still proves difficult for even state-of-the-art models, by presenting HellaSwag, a new challenge dataset. Though its questions are trivial for humans (>95% accuracy), state-of-the-art models struggle (<48%). We achieve this via Adversarial Filtering (AF), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. AF proves to be surprisingly robust. The key insight is to scale up the length and complexity of the dataset examples towards a critical 'Goldilocks' zone wherein generated text is ridiculous to humans, yet often misclassified by state-of-the-art models.   Our construction of HellaSwag, and its resulting difficulty, sheds light on the inner workings of deep pretrained models. More broadly, it suggests a new path forward for NLP research, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, so as to present ever-harder challenges.",
    "annotations_from_paper": {
        "Name": 1,
        "Link": 1,
        "HF_Link": 0,
        "License": 0,
        "Year": 1,
        "Language": 1,
        "Domain": 1,
        "Form": 1,
        "Collection_Style": 1,
        "Description": 1,
        "Volume": 1,
        "Unit": 1,
        "Ethical_Risks": 1,
        "Provider": 1,
        "Derived_From": 1,
        "Paper_Title": 1,
        "Paper_Link": 1,
        "Tokenized": 1,
        "Host": 1,
        "Access": 1,
        "Cost": 1,
        "Test_Split": 1,
        "Tasks": 1,
        "Venue_Title": 1,
        "Venue_Type": 1,
        "Venue_Name": 1,
        "Authors": 1,
        "Affiliations": 1,
        "Abstract": 1
    }
}