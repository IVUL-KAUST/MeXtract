{
    "Name": "FREEMmax",
    "Link": "https://github.com/FreEM-corpora/FreEMmax_OA",
    "HF_Link": "",
    "License": "custom",
    "Year": 2022,
    "Language": "fr",
    "Domain": [
        "web pages",
        "public datasets"
    ],
    "Form": "text",
    "Collection_Style": [
        "crawling",
        "manual curation"
    ],
    "Description": "FREEMmax is a large corpus of Early Modern French (16th-18th centuries), with some texts extending to the 1920s. It aggregates texts from various sources, including institutional databases, research projects, and web scraping, covering diverse genres like literature, correspondence, and plays.",
    "Volume": 185643482.0,
    "Unit": "tokens",
    "Ethical_Risks": "Low",
    "Provider": [
        "Inria",
        "Sorbonne Universite",
        "Universite de Geneve",
        "LIGM",
        "Universite Gustage Eiffel",
        "CNRS"
    ],
    "Derived_From": [
        "FRANTEXT",
        "Electronic Enlightenment",
        "Antonomaz project",
        "Acta Pacis Westphalicae",
        "Bibliotheques virtuelles humanistes",
        "Corpus electronique de la premiere modernite",
        "Conde project",
        "Corpus Descartes",
        "Bibliotheque dramatique",
        "Fabula numerica project",
        "Fonds Boissy",
        "Mercure Galant project",
        "Rousseau online project",
        "Sermo project",
        "The\u00b4a\u02c6tre classique project",
        "Wikisource",
        "Gallica"
    ],
    "Paper_Title": "From FREEM to D\u2019AlemBERT: a Large Corpus and a Language Model for Early Modern French",
    "Paper_Link": "https://arxiv.org/pdf/2202.09452",
    "Tokenized": false,
    "Host": "zenodo",
    "Access": "Free",
    "Cost": "",
    "Test_Split": false,
    "Tasks": [
        "language modeling"
    ],
    "Venue_Title": "arXiv",
    "Venue_Type": "preprint",
    "Venue_Name": "arXiv",
    "Authors": [
        "Simon Gabay",
        "Pedro Ortiz Suarez",
        "Alexandre Bartz",
        "Alix Chague",
        "Rachel Bawden",
        "Philippe Gambette",
        "Beno\u00eet Sagot"
    ],
    "Affiliations": [
        "Inria",
        "Sorbonne Universite",
        "Universite de Geneve",
        "LIGM",
        "Universite Gustage Eiffel",
        "CNRS"
    ],
    "Abstract": "Language models for historical states of language are becoming increasingly important to allow the optimal digitisation and analysis of old textual sources. Because these historical states are at the same time more complex to process and more scarce in the corpora available, specific efforts are necessary to train natural language processing (NLP) tools adapted to the data. In this paper, we present our efforts to develop NLP tools for Early Modern French (historical French from the 16$^\\text{th}$ to the 18$^\\text{th}$ centuries). We present the $\\text{FreEM}_{\\text{max}}$ corpus of Early Modern French and D'AlemBERT, a RoBERTa-based language model trained on $\\text{FreEM}_{\\text{max}}$. We evaluate the usefulness of D'AlemBERT by fine-tuning it on a part-of-speech tagging task, outperforming previous work on the test set. Importantly, we find evidence for the transfer learning capacity of the language model, since its performance on lesser-resourced time periods appears to have been boosted by the more resourced ones. We release D'AlemBERT and the open-sourced subpart of the $\\text{FreEM}_{\\text{max}}$ corpus.",
    "annotations_from_paper": {
        "Name": 1,
        "Link": 0,
        "HF_Link": 0,
        "License": 0,
        "Year": 1,
        "Language": 1,
        "Domain": 1,
        "Form": 1,
        "Collection_Style": 1,
        "Description": 1,
        "Volume": 1,
        "Unit": 1,
        "Ethical_Risks": 1,
        "Provider": 1,
        "Derived_From": 1,
        "Paper_Title": 1,
        "Paper_Link": 1,
        "Tokenized": 1,
        "Host": 1,
        "Access": 1,
        "Cost": 1,
        "Test_Split": 1,
        "Tasks": 1,
        "Venue_Title": 1,
        "Venue_Type": 1,
        "Venue_Name": 1,
        "Authors": 1,
        "Affiliations": 1,
        "Abstract": 1
    }
}