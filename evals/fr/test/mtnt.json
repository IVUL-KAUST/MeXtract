{
    "Name": "MTNT",
    "Link": "https://github.com/pmichel31415/mtnt",
    "HF_Link": "",
    "License": "MIT License",
    "Year": 2018,
    "Language": "multilingual",
    "Domain": [
        "social media",
        "commentary"
    ],
    "Form": "text",
    "Collection_Style": [
        "crawling",
        "human annotation"
    ],
    "Description": "A benchmark dataset for Machine Translation of Noisy Text (MTNT), consisting of noisy comments on Reddit and professionally sourced translations. It includes English comments translated into French and Japanese, as well as French and Japanese comments translated into English, on the order of 7k-37k sentences per language pair.",
    "Volume": 37930.0,
    "Unit": "sentences",
    "Ethical_Risks": "High",
    "Provider": [
        "Carnegie Mellon University"
    ],
    "Derived_From": [],
    "Paper_Title": "MTNT: A Testbed for Machine Translation of Noisy Text",
    "Paper_Link": "https://arxiv.org/pdf/1809.00388",
    "Tokenized": false,
    "Host": "GitHub",
    "Access": "Free",
    "Cost": "",
    "Test_Split": true,
    "Tasks": [
        "machine translation"
    ],
    "Venue_Title": "arXiv",
    "Venue_Type": "preprint",
    "Venue_Name": "",
    "Authors": [
        "Paul Michel",
        "Graham Neubig"
    ],
    "Affiliations": [
        "Language Technologies Institute",
        "Carnegie Mellon University"
    ],
    "Abstract": "Noisy or non-standard input text can cause disastrous mistranslations in most modern Machine Translation (MT) systems, and there has been growing research interest in creating noise-robust MT systems. However, as of yet there are no publicly available parallel corpora of with naturally occurring noisy inputs and translations, and thus previous work has resorted to evaluating on synthetically created datasets. In this paper, we propose a benchmark dataset for Machine Translation of Noisy Text (MTNT), consisting of noisy comments on Reddit (www.reddit.com) and professionally sourced translations. We commissioned translations of English comments into French and Japanese, as well as French and Japanese comments into English, on the order of 7k-37k sentences per language pair. We qualitatively and quantitatively examine the types of noise included in this dataset, then demonstrate that existing MT models fail badly on a number of noise-related phenomena, even after performing adaptation on a small training set of in-domain data. This indicates that this dataset can provide an attractive testbed for methods tailored to handling noisy text in MT. The data is publicly available at www.cs.cmu.edu/~pmichel1/mtnt/.",
    "annotations_from_paper": {
        "Name": 1,
        "Link": 0,
        "HF_Link": 0,
        "License": 0,
        "Year": 1,
        "Language": 1,
        "Domain": 1,
        "Form": 1,
        "Collection_Style": 1,
        "Description": 1,
        "Volume": 1,
        "Unit": 1,
        "Ethical_Risks": 1,
        "Provider": 1,
        "Derived_From": 1,
        "Paper_Title": 1,
        "Paper_Link": 1,
        "Tokenized": 1,
        "Host": 0,
        "Access": 1,
        "Cost": 1,
        "Test_Split": 1,
        "Tasks": 1,
        "Venue_Title": 1,
        "Venue_Type": 1,
        "Venue_Name": 1,
        "Authors": 1,
        "Affiliations": 1,
        "Abstract": 1
    }
}