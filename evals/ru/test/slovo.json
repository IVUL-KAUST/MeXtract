{
    "Name": "Slovo",
    "Link": "https://github.com/hukenovs/slovo",
    "HF_Link": "",
    "License": "custom",
    "Year": 2024,
    "Language": "ru",
    "Domain": [
        "web pages"
    ],
    "Form": "videos",
    "Collection_Style": [
        "manual curation",
        "human annotation"
    ],
    "Description": "A Russian Sign Language (RSL) video dataset containing 20,000 FullHD recordings of 1,000 isolated RSL gestures from 194 signers. The data was collected via crowdsourcing platforms and covers frequently used words like food, animals, emotions, and colors.",
    "Volume": 20000.0,
    "Unit": "sentences",
    "Ethical_Risks": "Low",
    "Provider": [
        "SaluteDevices"
    ],
    "Derived_From": [],
    "Paper_Title": "Slovo: Russian Sign Language Dataset",
    "Paper_Link": "https://arxiv.org/pdf/2305.14527",
    "Tokenized": false,
    "Host": "GitHub",
    "Access": "Free",
    "Cost": "",
    "Test_Split": true,
    "Tasks": [
        "sign language recognition"
    ],
    "Venue_Title": "arXiv",
    "Venue_Type": "preprint",
    "Venue_Name": "",
    "Authors": [
        "Kapitanov Alexander",
        "Kvanchiani Karina",
        "Nagaev Alexander",
        "Petrova Elizaveta"
    ],
    "Affiliations": [
        "SaluteDevices, Russia"
    ],
    "Abstract": "One of the main challenges of the sign language recognition task is the difficulty of collecting a suitable dataset due to the gap between hard-of-hearing and hearing societies. In addition, the sign language in each country differs significantly, which obliges the creation of new data for each of them. This paper presents the Russian Sign Language (RSL) video dataset Slovo, produced using crowdsourcing platforms. The dataset contains 20,000 FullHD recordings, divided into 1,000 classes of isolated RSL gestures received by 194 signers. We also provide the entire dataset creation pipeline, from data collection to video annotation, with the following demo application. Several neural networks are trained and evaluated on the Slovo to demonstrate its teaching ability. Proposed data and pre-trained models are publicly available.",
    "annotations_from_paper": {
        "Name": 1,
        "Link": 1,
        "HF_Link": 0,
        "License": 0,
        "Year": 1,
        "Language": 1,
        "Domain": 1,
        "Form": 1,
        "Collection_Style": 1,
        "Description": 1,
        "Volume": 1,
        "Unit": 1,
        "Ethical_Risks": 1,
        "Provider": 1,
        "Derived_From": 1,
        "Paper_Title": 1,
        "Paper_Link": 1,
        "Tokenized": 1,
        "Host": 1,
        "Access": 1,
        "Cost": 1,
        "Test_Split": 1,
        "Tasks": 1,
        "Venue_Title": 1,
        "Venue_Type": 1,
        "Venue_Name": 1,
        "Authors": 1,
        "Affiliations": 1,
        "Abstract": 1
    }
}